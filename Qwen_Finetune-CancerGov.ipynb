{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9d7582-2924-467e-90a3-24c8b61d3fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.cuda.get_device_name(0))  # GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901649d4-6c5b-4c06-975f-286a3cde04cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 11:39:23.545668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-30 11:39:23.545689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-30 11:39:23.546475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-30 11:39:23.550293: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-30 11:39:24.094016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7be530-a9ac-49ab-bcbe-7dab8851414f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>focus</th>\n",
       "      <th>qa_pair_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_type</th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000013_2</td>\n",
       "      <td>Chronic Eosinophilic Leukemia</td>\n",
       "      <td>1</td>\n",
       "      <td>0000013_2-1</td>\n",
       "      <td>information</td>\n",
       "      <td>What is (are) Chronic Eosinophilic Leukemia ?</td>\n",
       "      <td>Key Points\\n                    - Chronic eosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000013_2</td>\n",
       "      <td>Chronic Eosinophilic Leukemia</td>\n",
       "      <td>2</td>\n",
       "      <td>0000013_2-2</td>\n",
       "      <td>symptoms</td>\n",
       "      <td>What are the symptoms of Chronic Eosinophilic ...</td>\n",
       "      <td>Signs and symptoms of chronic eosinophilic leu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000013_2</td>\n",
       "      <td>Chronic Eosinophilic Leukemia</td>\n",
       "      <td>3</td>\n",
       "      <td>0000013_2-3</td>\n",
       "      <td>treatment</td>\n",
       "      <td>What are the treatments for Chronic Eosinophil...</td>\n",
       "      <td>Treatment of chronic eosinophilic leukemia may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000003_5</td>\n",
       "      <td>Childhood Soft Tissue Sarcoma</td>\n",
       "      <td>1</td>\n",
       "      <td>0000003_5-1</td>\n",
       "      <td>information</td>\n",
       "      <td>What is (are) Childhood Soft Tissue Sarcoma ?</td>\n",
       "      <td>Key Points\\n\\t\\t\\t\\t\\t\\t\\t                    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000003_5</td>\n",
       "      <td>Childhood Soft Tissue Sarcoma</td>\n",
       "      <td>2</td>\n",
       "      <td>0000003_5-2</td>\n",
       "      <td>susceptibility</td>\n",
       "      <td>Who is at risk for Childhood Soft Tissue Sarco...</td>\n",
       "      <td>Having certain diseases and inherited disorder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>0000014_1</td>\n",
       "      <td>Endometrial Cancer</td>\n",
       "      <td>4</td>\n",
       "      <td>0000014_1-4</td>\n",
       "      <td>exams and tests</td>\n",
       "      <td>How to diagnose Endometrial Cancer ?</td>\n",
       "      <td>Tests that examine the endometrium are used to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>0000014_1</td>\n",
       "      <td>Endometrial Cancer</td>\n",
       "      <td>5</td>\n",
       "      <td>0000014_1-5</td>\n",
       "      <td>outlook</td>\n",
       "      <td>What is the outlook for Endometrial Cancer ?</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>0000014_1</td>\n",
       "      <td>Endometrial Cancer</td>\n",
       "      <td>6</td>\n",
       "      <td>0000014_1-6</td>\n",
       "      <td>stages</td>\n",
       "      <td>What are the stages of Endometrial Cancer ?</td>\n",
       "      <td>Key Points\\n                    - After endome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>0000014_1</td>\n",
       "      <td>Endometrial Cancer</td>\n",
       "      <td>7</td>\n",
       "      <td>0000014_1-7</td>\n",
       "      <td>treatment</td>\n",
       "      <td>What are the treatments for Endometrial Cancer ?</td>\n",
       "      <td>Key Points\\n                    - There are di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0000014_1</td>\n",
       "      <td>Endometrial Cancer</td>\n",
       "      <td>8</td>\n",
       "      <td>0000014_1-8</td>\n",
       "      <td>research</td>\n",
       "      <td>what research (or clinical trials) is being do...</td>\n",
       "      <td>New types of treatment are being tested in cli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>729 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_id                          focus qa_pair_id  question_id  \\\n",
       "0     0000013_2  Chronic Eosinophilic Leukemia          1  0000013_2-1   \n",
       "1     0000013_2  Chronic Eosinophilic Leukemia          2  0000013_2-2   \n",
       "2     0000013_2  Chronic Eosinophilic Leukemia          3  0000013_2-3   \n",
       "3     0000003_5  Childhood Soft Tissue Sarcoma          1  0000003_5-1   \n",
       "4     0000003_5  Childhood Soft Tissue Sarcoma          2  0000003_5-2   \n",
       "..          ...                            ...        ...          ...   \n",
       "724   0000014_1             Endometrial Cancer          4  0000014_1-4   \n",
       "725   0000014_1             Endometrial Cancer          5  0000014_1-5   \n",
       "726   0000014_1             Endometrial Cancer          6  0000014_1-6   \n",
       "727   0000014_1             Endometrial Cancer          7  0000014_1-7   \n",
       "728   0000014_1             Endometrial Cancer          8  0000014_1-8   \n",
       "\n",
       "       question_type                                      question_text  \\\n",
       "0        information      What is (are) Chronic Eosinophilic Leukemia ?   \n",
       "1           symptoms  What are the symptoms of Chronic Eosinophilic ...   \n",
       "2          treatment  What are the treatments for Chronic Eosinophil...   \n",
       "3        information      What is (are) Childhood Soft Tissue Sarcoma ?   \n",
       "4     susceptibility  Who is at risk for Childhood Soft Tissue Sarco...   \n",
       "..               ...                                                ...   \n",
       "724  exams and tests               How to diagnose Endometrial Cancer ?   \n",
       "725          outlook       What is the outlook for Endometrial Cancer ?   \n",
       "726           stages        What are the stages of Endometrial Cancer ?   \n",
       "727        treatment   What are the treatments for Endometrial Cancer ?   \n",
       "728         research  what research (or clinical trials) is being do...   \n",
       "\n",
       "                                           answer_text  \n",
       "0    Key Points\\n                    - Chronic eosi...  \n",
       "1    Signs and symptoms of chronic eosinophilic leu...  \n",
       "2    Treatment of chronic eosinophilic leukemia may...  \n",
       "3    Key Points\\n\\t\\t\\t\\t\\t\\t\\t                    ...  \n",
       "4    Having certain diseases and inherited disorder...  \n",
       "..                                                 ...  \n",
       "724  Tests that examine the endometrium are used to...  \n",
       "725  Certain factors affect prognosis (chance of re...  \n",
       "726  Key Points\\n                    - After endome...  \n",
       "727  Key Points\\n                    - There are di...  \n",
       "728  New types of treatment are being tested in cli...  \n",
       "\n",
       "[729 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/Pedrampedram/CancerGov_QA/data/train-00000-of-00001-95d10b894a8afa4d.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5a0d65-aceb-42a3-85a1-9baad6fff010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (583, 7)\n",
      "Shape of validation set: (73, 7)\n",
      "Shape of test set: (73, 7)\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Shape of training set: {train_df.shape}\")\n",
    "print(f\"Shape of validation set: {validation_df.shape}\")\n",
    "print(f\"Shape of test set: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8025a621-85b1-4ed6-ba76-a21531fb7c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2835e679124d50b48ba752c76bb498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b545b8d-73be-48c9-99c2-2cf3fad3dfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# === Qwen3-32B 4-bit GPU setup and generation ===\\n\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nimport torch\\n\\n# --- Step 1: BitsAndBytes 4-bit config ---\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=False,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.bfloat16,\\n)\\n\\n# --- Step 2: Load tokenizer ---\\nmodel_dir = \"Qwen/Qwen3-32B\"\\ntokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\\n\\n# --- Step 3: Load model on GPU(s) ---\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_dir,\\n    quantization_config=bnb_config,\\n    device_map=\"auto\",       # Automatically distributes layers across GPUs\\n    torch_dtype=torch.bfloat16,\\n    trust_remote_code=True,\\n    offload_folder=\"offload\",  # Optional: offload layers to CPU if GPU memory is low\\n    offload_state_dict=True\\n)\\n\\n# Disable caching to save GPU memory\\nmodel.config.use_cache = False\\nmodel.config.pretraining_tp = 1\\n\\n# --- Step 4: Check GPU devices ---\\nprint(\"CUDA available:\", torch.cuda.is_available())\\nprint(\"Number of GPUs:\", torch.cuda.device_count())\\nprint(\"Model device map:\", model.hf_device_map)\\n\\n# --- Step 5: Generate text ---\\nprompt = \"Once upon a time in a futuristic city,\"\\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move inputs to GPU\\n\\n# Generate output\\nwith torch.no_grad():\\n    outputs = model.generate(\\n        **inputs,\\n        max_new_tokens=100,\\n        do_sample=True,\\n        top_p=0.95,\\n        temperature=0.8\\n    )\\n\\n# Decode and print\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(\"=== Generated Text ===\")\\nprint(generated_text)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# === Qwen3-32B 4-bit GPU setup and generation ===\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# --- Step 1: BitsAndBytes 4-bit config ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- Step 2: Load tokenizer ---\n",
    "model_dir = \"Qwen/Qwen3-32B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "# --- Step 3: Load model on GPU(s) ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",       # Automatically distributes layers across GPUs\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    offload_folder=\"offload\",  # Optional: offload layers to CPU if GPU memory is low\n",
    "    offload_state_dict=True\n",
    ")\n",
    "\n",
    "# Disable caching to save GPU memory\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# --- Step 4: Check GPU devices ---\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Model device map:\", model.hf_device_map)\n",
    "\n",
    "# --- Step 5: Generate text ---\n",
    "prompt = \"Once upon a time in a futuristic city,\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move inputs to GPU\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Generated Text ===\")\n",
    "print(generated_text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a08c679d-94bb-4dee-904f-7f922b5a1551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Text ===\n",
      "Once upon a time in a futuristic city, there was a young boy named Arnie. He lived a normal life until one day, while playing his favorite game, he was attacked by a giant squid, causing him to fall into a deep well. Arnie was saved by a kindly fisherman, who gave him a magic potion that helped him recover.\n",
      "\n",
      "From that day on, Arnie always wore a special necklace around his neck, which was named after the creature that saved his life. The necklace had a symbol of a dragonfly on it\n",
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "Model device map: {'': 1}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# --- Step 1: 4-bit config for VRAM efficiency ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- Step 2: Load tokenizer ---\n",
    "model_dir = \"Qwen/Qwen2-0.5B\"  # Choose the 0.5B model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "# --- Step 3: Load model across multiple GPUs ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",       # Automatically shard across all available GPUs\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Disable caching to save VRAM\n",
    "model.config.use_cache = False\n",
    "\n",
    "# --- Step 4: Determine first device for inputs ---\n",
    "first_device = list(model.hf_device_map.values())[0]\n",
    "\n",
    "# --- Step 5: Generate text ---\n",
    "prompt = \"Once upon a time in a futuristic city,\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(first_device)  # Place on first shard\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Generated Text ===\")\n",
    "print(generated_text)\n",
    "\n",
    "# --- Step 6: Optional info ---\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Model device map:\", model.hf_device_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f609fab3-6675-4484-acf1-254ced98c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_qa(row):\n",
    "    return f\"Question: {row['question_text']}\\nAnswer: {row['answer_text']}\\n\"\n",
    "\n",
    "train_texts = train_df.apply(combine_qa, axis=1).tolist()\n",
    "val_texts = validation_df.apply(combine_qa, axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "792f8992-6e94-4dd1-9723-da3f053ab4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings = tokenize_function(val_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1e99d79-60f0-4158-8ef6-82df586de2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",        # Multi-GPU\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ab417f8-2182-4c38-b850-71343098a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "        self.attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx]),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask[idx]),\n",
    "            \"labels\": torch.tensor(self.input_ids[idx])\n",
    "        }\n",
    "\n",
    "train_dataset = QADataset(train_encodings)\n",
    "val_dataset = QADataset(val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6191172-f8a7-4755-8f12-bdd65e37a702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=False,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.bfloat16,\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    quantization_config=bnb_config,\\n    device_map=\"auto\",  # Automatically shard across GPUs\\n    torch_dtype=torch.bfloat16,\\n    trust_remote_code=True\\n)\\n\\nmodel.config.use_cache = False\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically shard across GPUs\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10df7a40-811f-4e61-befd-137e6dcc9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")  # your GPU\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": device},  # pin model to single GPU\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f38454-50f1-47bf-a793-612d3f47da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # adjust if needed\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b3fba72-b92c-4715-a9c6-ecf2ddb666cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen2-0.5B-finetuned-cancergov\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_total_limit=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2de6697d-d79d-47d1-8df2-a8e9ffcbfc5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 11:39:37] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 11:39:37] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 11:39:37] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 11:39:38] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 11:39:38] CPU Model on constant consumption mode: AMD Ryzen 7 5800X 8-Core Processor\n",
      "[codecarbon WARNING @ 11:39:38] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 11:39:38] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 11:39:38] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 11:39:38] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 11:39:38] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 11:39:38]   Platform system: Linux-5.15.0-153-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 11:39:38]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 11:39:38]   CodeCarbon version: 3.0.5\n",
      "[codecarbon INFO @ 11:39:38]   Available RAM : 62.701 GB\n",
      "[codecarbon INFO @ 11:39:38]   CPU count: 16 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 11:39:38]   CPU model: AMD Ryzen 7 5800X 8-Core Processor\n",
      "[codecarbon INFO @ 11:39:38]   GPU count: 2\n",
      "[codecarbon INFO @ 11:39:38]   GPU model: 2 x NVIDIA GeForce RTX 3090\n",
      "[codecarbon INFO @ 11:39:41] Emissions data (if any) will be saved to file /home/bobby/Cancer4VN/qwen2-0.5B-finetuned-cancergov/emissions.csv\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f5c1e8-deb4-47e9-b159-e3a6665b2620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobbyhieubui\u001b[0m (\u001b[33mbobbyhieubui-university-of-wisconsin-madison\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bobby/Cancer4VN/wandb/run-20250930_113943-1chyueps</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/1chyueps' target=\"_blank\">peachy-night-5</a></strong> to <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface' target=\"_blank\">https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/1chyueps' target=\"_blank\">https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/1chyueps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111/111 00:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.624100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 11:39:59] Energy consumed for RAM : 0.000086 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:40:00] Delta energy consumed for CPU with cpu_load : 0.000045 kWh, power : 10.525678919699999 W\n",
      "[codecarbon INFO @ 11:40:00] Energy consumed for All CPU : 0.000045 kWh\n",
      "[codecarbon INFO @ 11:40:00] Energy consumed for all GPUs : 0.001269 kWh. Total GPU Power : 285.44291136344515 W\n",
      "[codecarbon INFO @ 11:40:00] 0.001401 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:14] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:40:15] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5248433058125 W\n",
      "[codecarbon INFO @ 11:40:15] Energy consumed for All CPU : 0.000088 kWh\n",
      "[codecarbon INFO @ 11:40:15] Energy consumed for all GPUs : 0.002515 kWh. Total GPU Power : 299.0781584622943 W\n",
      "[codecarbon INFO @ 11:40:15] 0.002769 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:29] Energy consumed for RAM : 0.000247 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:40:30] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523872767187504 W\n",
      "[codecarbon INFO @ 11:40:30] Energy consumed for All CPU : 0.000130 kWh\n",
      "[codecarbon INFO @ 11:40:30] Energy consumed for all GPUs : 0.003774 kWh. Total GPU Power : 302.33963773260416 W\n",
      "[codecarbon INFO @ 11:40:30] 0.004152 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:40:43] Energy consumed for RAM : 0.000324 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:40:44] Delta energy consumed for CPU with cpu_load : 0.000041 kWh, power : 10.520022642281248 W\n",
      "[codecarbon INFO @ 11:40:44] Energy consumed for All CPU : 0.000171 kWh\n",
      "[codecarbon INFO @ 11:40:44] Energy consumed for all GPUs : 0.004952 kWh. Total GPU Power : 294.2473736583473 W\n",
      "[codecarbon INFO @ 11:40:44] 0.005447 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=111, training_loss=0.9384751899822338, metrics={'train_runtime': 60.8301, 'train_samples_per_second': 28.752, 'train_steps_per_second': 1.825, 'total_flos': 1925865778839552.0, 'train_loss': 0.9384751899822338, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d15764a-583b-45c9-9d97-cb8e0594fea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuning complete. LoRA weights saved.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./qwen2-0.5B-lora-finetuned-cancergov\")\n",
    "print(\"✅ Fine-tuning complete. LoRA weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85a6c182-ae6d-44c6-93e1-9a288d83df41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport evaluate\\n\\n# Load BLEU metric\\nbleu = evaluate.load(\"bleu\")\\n\\npredictions = []\\nreferences = []\\n\\nfor idx in range(len(val_dataset)):\\n    input_ids = val_dataset[idx][\"input_ids\"].unsqueeze(0).to(model.device)\\n    attention_mask = val_dataset[idx][\"attention_mask\"].unsqueeze(0).to(model.device)\\n\\n    with torch.no_grad():\\n        output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512)\\n\\n    pred_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\\n    ref_text = tokenizer.decode(val_dataset[idx][\"labels\"], skip_special_tokens=True)\\n\\n    predictions.append(pred_text)       # <-- full string\\n    references.append([ref_text])       # <-- list of strings\\n\\n# Compute BLEU\\nbleu_score = bleu.compute(predictions=predictions, references=references)\\nprint(\"BLEU score:\", bleu_score[\"bleu\"])\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for idx in range(len(val_dataset)):\n",
    "    input_ids = val_dataset[idx][\"input_ids\"].unsqueeze(0).to(model.device)\n",
    "    attention_mask = val_dataset[idx][\"attention_mask\"].unsqueeze(0).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512)\n",
    "\n",
    "    pred_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    ref_text = tokenizer.decode(val_dataset[idx][\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred_text)       # <-- full string\n",
    "    references.append([ref_text])       # <-- list of strings\n",
    "\n",
    "# Compute BLEU\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_score[\"bleu\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ccb7ba-b7f3-427c-b78a-bb747b80c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.856337482013129\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader to batch the validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "predictions, references = [], []\n",
    "\n",
    "for batch in val_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    # Decode predictions\n",
    "    batch_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Decode references (fix -100s first)\n",
    "    batch_refs = []\n",
    "    for labels in batch[\"labels\"]:\n",
    "        labels = [tok if tok != -100 else tokenizer.pad_token_id for tok in labels]\n",
    "        batch_refs.append([tokenizer.decode(labels, skip_special_tokens=True)])\n",
    "\n",
    "    predictions.extend(batch_preds)\n",
    "    references.extend(batch_refs)\n",
    "\n",
    "# Compute BLEU\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_score[\"bleu\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94125b-1705-498d-af17-ef2565268323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen32b)",
   "language": "python",
   "name": "qwen32b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
