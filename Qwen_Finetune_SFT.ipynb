{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0464c7e-a8d1-4980-8ebc-db5979b8a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.cuda.get_device_name(0))  # GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23f74e6-ad79-4a7b-91bf-d3aa9952c1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 11:15:41.852097: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-05 11:15:41.852118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-05 11:15:41.852898: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-05 11:15:41.856727: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-05 11:15:42.383393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ac27a6-5e95-436c-8397-cfd3d914c3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the survival rate for lung cancer pati...</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is lung cancer diagnosed?</td>\n",
       "      <td>Air pollution, particularly fine particulate m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is lung cancer hereditary?</td>\n",
       "      <td>Air pollution, particularly fine particulate m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the stages of lung cancer?</td>\n",
       "      <td>Targeted therapies focus on specific genetic m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are there clinical trials for lung cancer trea...</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>What treatments are available for Stage 1 lung...</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Are there alternative therapies for lung cancer?</td>\n",
       "      <td>The survival rate depends on the stage at diag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Are there clinical trials for lung cancer trea...</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>How is lung cancer diagnosed?</td>\n",
       "      <td>Some alternative therapies like acupuncture ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>How is lung cancer diagnosed?</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  \\\n",
       "0     What is the survival rate for lung cancer pati...   \n",
       "1                         How is lung cancer diagnosed?   \n",
       "2                            Is lung cancer hereditary?   \n",
       "3                   What are the stages of lung cancer?   \n",
       "4     Are there clinical trials for lung cancer trea...   \n",
       "...                                                 ...   \n",
       "2995  What treatments are available for Stage 1 lung...   \n",
       "2996   Are there alternative therapies for lung cancer?   \n",
       "2997  Are there clinical trials for lung cancer trea...   \n",
       "2998                      How is lung cancer diagnosed?   \n",
       "2999                      How is lung cancer diagnosed?   \n",
       "\n",
       "                                                 output  \n",
       "0     Numerous clinical trials are exploring targete...  \n",
       "1     Air pollution, particularly fine particulate m...  \n",
       "2     Air pollution, particularly fine particulate m...  \n",
       "3     Targeted therapies focus on specific genetic m...  \n",
       "4     Numerous clinical trials are exploring targete...  \n",
       "...                                                 ...  \n",
       "2995  Numerous clinical trials are exploring targete...  \n",
       "2996  The survival rate depends on the stage at diag...  \n",
       "2997  Numerous clinical trials are exploring targete...  \n",
       "2998  Some alternative therapies like acupuncture ma...  \n",
       "2999  Numerous clinical trials are exploring targete...  \n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hf://datasets/SandeepKumarRudhravaram/Lung_Cancer_QA/Regenerated_Lung_Cancer_QA_Dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b842f78-0713-492d-91d4-22fc5e356197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (2400, 2)\n",
      "Shape of validation set: (300, 2)\n",
      "Shape of test set: (300, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Shape of training set: {train_df.shape}\")\n",
    "print(f\"Shape of validation set: {validation_df.shape}\")\n",
    "print(f\"Shape of test set: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f2fa30-87e7-4519-802a-7f44ab90eee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', '__index_level_0__'],\n",
      "        num_rows: 2400\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output', '__index_level_0__'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input', 'output', '__index_level_0__'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset   = Dataset.from_pandas(validation_df)\n",
    "test_dataset  = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ddbbfce-c615-4c1e-99d1-6ff4f8a475f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87c0684288445a79e7f18a19558b58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d1f96da7b44ed8bf409095a248f252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b482fcc859e6413dbb722b403fc99559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_example(example):\n",
    "    prompt = f\"### Question:\\n{example['input']}\\n\\n### Answer:\\n\"\n",
    "    return {\"text\": prompt + example['output']}\n",
    "\n",
    "dataset = dataset.map(format_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9e1afe-df26-4328-903e-683bd756387c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ed0935f8fa4baebface106c7c0dee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480a435e33b7479b96cddf5afa83e4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498e62549d184e59852ad4ec596a7076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "# Load tokenizer first\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Add a pad token if missing (Qwen models often don’t have one)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\"\"\"\n",
    "# Define tokenize function (example if you have separate question/answer columns)\n",
    "def tokenize(batch):\n",
    "    enc = tokenizer(\n",
    "        f\"Question: {batch['input']}\\nAnswer:\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    answer_enc = tokenizer(\n",
    "        batch[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    labels = [\n",
    "        tok if tok != tokenizer.pad_token_id else -100\n",
    "        for tok in answer_enc[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n",
    "\"\"\"\n",
    "def tokenize(batch):\n",
    "    prompt = f\"Question: {batch['input']}\\nAnswer:\"\n",
    "    answer = batch[\"output\"]\n",
    "\n",
    "    prompt_enc = tokenizer(prompt, truncation=True, max_length=256)\n",
    "    answer_enc = tokenizer(answer, truncation=True, max_length=256)\n",
    "\n",
    "    input_ids = prompt_enc['input_ids'] + answer_enc['input_ids']\n",
    "    labels = [-100] * len(prompt_enc['input_ids']) + answer_enc['input_ids']\n",
    "\n",
    "    # Pad sequences to 512 tokens\n",
    "    pad_length = 512 - len(input_ids)\n",
    "    if pad_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * pad_length\n",
    "        labels = labels + [-100] * pad_length\n",
    "    else:\n",
    "        input_ids = input_ids[:512]\n",
    "        labels = labels[:512]\n",
    "\n",
    "    attention_mask = [1 if id != tokenizer.pad_token_id else 0 for id in input_ids]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "# Apply to dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c8a297-d09b-4545-bfd2-f1f1d50ea2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,  # <-- force FP16\n",
    ").cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64131a00-1b85-4f38-a7ea-a7d14a69a844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_406323/2954534133.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon WARNING @ 11:15:50] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 11:15:50] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 11:15:50] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 11:15:51] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 11:15:51] CPU Model on constant consumption mode: AMD Ryzen 7 5800X 8-Core Processor\n",
      "[codecarbon WARNING @ 11:15:51] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 11:15:51] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 11:15:51] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 11:15:51] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 11:15:51] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 11:15:51]   Platform system: Linux-5.15.0-153-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 11:15:51]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 11:15:51]   CodeCarbon version: 3.0.5\n",
      "[codecarbon INFO @ 11:15:51]   Available RAM : 62.701 GB\n",
      "[codecarbon INFO @ 11:15:51]   CPU count: 16 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 11:15:52]   CPU model: AMD Ryzen 7 5800X 8-Core Processor\n",
      "[codecarbon INFO @ 11:15:52]   GPU count: 2\n",
      "[codecarbon INFO @ 11:15:52]   GPU model: 2 x NVIDIA GeForce RTX 3090\n",
      "[codecarbon INFO @ 11:15:55] Emissions data (if any) will be saved to file /home/bobby/Cancer4VN/qwen-sft-final/emissions.csv\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobbyhieubui\u001b[0m (\u001b[33mbobbyhieubui-university-of-wisconsin-madison\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bobby/Cancer4VN/wandb/run-20251005_111556-d1upct5h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/d1upct5h' target=\"_blank\">dashing-frost-13</a></strong> to <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface' target=\"_blank\">https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/d1upct5h' target=\"_blank\">https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/d1upct5h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 09:06, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>116.529200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 11:16:12] Energy consumed for RAM : 0.000086 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:16:12] Delta energy consumed for CPU with cpu_load : 0.000045 kWh, power : 10.5240393384 W\n",
      "[codecarbon INFO @ 11:16:12] Energy consumed for All CPU : 0.000045 kWh\n",
      "[codecarbon INFO @ 11:16:12] Energy consumed for all GPUs : 0.001364 kWh. Total GPU Power : 306.6665698549011 W\n",
      "[codecarbon INFO @ 11:16:12] 0.001495 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:16:27] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:16:27] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52331469153125 W\n",
      "[codecarbon INFO @ 11:16:27] Energy consumed for All CPU : 0.000088 kWh\n",
      "[codecarbon INFO @ 11:16:27] Energy consumed for all GPUs : 0.002705 kWh. Total GPU Power : 321.9903391827555 W\n",
      "[codecarbon INFO @ 11:16:27] 0.002959 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:16:42] Energy consumed for RAM : 0.000247 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:16:42] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523167259718749 W\n",
      "[codecarbon INFO @ 11:16:42] Energy consumed for All CPU : 0.000130 kWh\n",
      "[codecarbon INFO @ 11:16:42] Energy consumed for all GPUs : 0.004048 kWh. Total GPU Power : 322.40753851239276 W\n",
      "[codecarbon INFO @ 11:16:42] 0.004425 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:16:57] Energy consumed for RAM : 0.000328 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:16:57] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524650053312502 W\n",
      "[codecarbon INFO @ 11:16:57] Energy consumed for All CPU : 0.000172 kWh\n",
      "[codecarbon INFO @ 11:16:57] Energy consumed for all GPUs : 0.005401 kWh. Total GPU Power : 324.74910271191163 W\n",
      "[codecarbon INFO @ 11:16:57] 0.005901 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:17:12] Energy consumed for RAM : 0.000408 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:17:12] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523739126468751 W\n",
      "[codecarbon INFO @ 11:17:12] Energy consumed for All CPU : 0.000215 kWh\n",
      "[codecarbon INFO @ 11:17:12] Energy consumed for all GPUs : 0.006763 kWh. Total GPU Power : 327.06898314487245 W\n",
      "[codecarbon INFO @ 11:17:12] 0.007386 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:17:27] Energy consumed for RAM : 0.000489 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:17:27] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523513371875 W\n",
      "[codecarbon INFO @ 11:17:27] Energy consumed for All CPU : 0.000257 kWh\n",
      "[codecarbon INFO @ 11:17:27] Energy consumed for all GPUs : 0.008122 kWh. Total GPU Power : 326.3326259206292 W\n",
      "[codecarbon INFO @ 11:17:27] 0.008868 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:17:42] Energy consumed for RAM : 0.000569 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:17:42] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523801502375001 W\n",
      "[codecarbon INFO @ 11:17:42] Energy consumed for All CPU : 0.000300 kWh\n",
      "[codecarbon INFO @ 11:17:42] Energy consumed for all GPUs : 0.009487 kWh. Total GPU Power : 327.6362774943632 W\n",
      "[codecarbon INFO @ 11:17:42] 0.010356 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:17:57] Energy consumed for RAM : 0.000650 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:17:57] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.534492051125001 W\n",
      "[codecarbon INFO @ 11:17:57] Energy consumed for All CPU : 0.000342 kWh\n",
      "[codecarbon INFO @ 11:17:57] Energy consumed for all GPUs : 0.010756 kWh. Total GPU Power : 304.49174398758004 W\n",
      "[codecarbon INFO @ 11:17:57] 0.011747 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:17:57] 0.019927 g.CO2eq/s mean an estimation of 628.4099153066289 kg.CO2eq/year\n",
      "[codecarbon INFO @ 11:18:12] Energy consumed for RAM : 0.000730 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:18:12] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52359594715625 W\n",
      "[codecarbon INFO @ 11:18:12] Energy consumed for All CPU : 0.000384 kWh\n",
      "[codecarbon INFO @ 11:18:12] Energy consumed for all GPUs : 0.012123 kWh. Total GPU Power : 328.1647242786846 W\n",
      "[codecarbon INFO @ 11:18:12] 0.013237 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:18:27] Energy consumed for RAM : 0.000811 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:18:27] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52301539821875 W\n",
      "[codecarbon INFO @ 11:18:27] Energy consumed for All CPU : 0.000427 kWh\n",
      "[codecarbon INFO @ 11:18:27] Energy consumed for all GPUs : 0.013509 kWh. Total GPU Power : 332.7541007004985 W\n",
      "[codecarbon INFO @ 11:18:27] 0.014746 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:18:42] Energy consumed for RAM : 0.000891 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:18:42] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5240715951875 W\n",
      "[codecarbon INFO @ 11:18:42] Energy consumed for All CPU : 0.000469 kWh\n",
      "[codecarbon INFO @ 11:18:42] Energy consumed for all GPUs : 0.014890 kWh. Total GPU Power : 331.5265162014225 W\n",
      "[codecarbon INFO @ 11:18:42] 0.016250 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:18:57] Energy consumed for RAM : 0.000972 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:18:57] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5233748703125 W\n",
      "[codecarbon INFO @ 11:18:57] Energy consumed for All CPU : 0.000511 kWh\n",
      "[codecarbon INFO @ 11:18:57] Energy consumed for all GPUs : 0.016263 kWh. Total GPU Power : 329.5782598458691 W\n",
      "[codecarbon INFO @ 11:18:57] 0.017746 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:19:12] Energy consumed for RAM : 0.001052 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:19:12] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52329672471875 W\n",
      "[codecarbon INFO @ 11:19:12] Energy consumed for All CPU : 0.000554 kWh\n",
      "[codecarbon INFO @ 11:19:12] Energy consumed for all GPUs : 0.017640 kWh. Total GPU Power : 330.41031425290413 W\n",
      "[codecarbon INFO @ 11:19:12] 0.019246 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:19:27] Energy consumed for RAM : 0.001133 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:19:27] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523374799437505 W\n",
      "[codecarbon INFO @ 11:19:27] Energy consumed for All CPU : 0.000596 kWh\n",
      "[codecarbon INFO @ 11:19:27] Energy consumed for all GPUs : 0.019008 kWh. Total GPU Power : 328.60705507905595 W\n",
      "[codecarbon INFO @ 11:19:27] 0.020737 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:19:42] Energy consumed for RAM : 0.001213 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:19:42] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523903812800002 W\n",
      "[codecarbon INFO @ 11:19:42] Energy consumed for All CPU : 0.000639 kWh\n",
      "[codecarbon INFO @ 11:19:42] Energy consumed for all GPUs : 0.020273 kWh. Total GPU Power : 303.58785039572285 W\n",
      "[codecarbon INFO @ 11:19:42] 0.022124 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:19:57] Energy consumed for RAM : 0.001294 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:19:57] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.527914177812503 W\n",
      "[codecarbon INFO @ 11:19:57] Energy consumed for All CPU : 0.000681 kWh\n",
      "[codecarbon INFO @ 11:19:57] Energy consumed for all GPUs : 0.021646 kWh. Total GPU Power : 329.796709939678 W\n",
      "[codecarbon INFO @ 11:19:57] 0.023621 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:19:57] 0.020311 g.CO2eq/s mean an estimation of 640.516638584434 kg.CO2eq/year\n",
      "[codecarbon INFO @ 11:20:12] Energy consumed for RAM : 0.001374 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:20:12] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524299617781251 W\n",
      "[codecarbon INFO @ 11:20:12] Energy consumed for All CPU : 0.000723 kWh\n",
      "[codecarbon INFO @ 11:20:12] Energy consumed for all GPUs : 0.023026 kWh. Total GPU Power : 331.07309512637613 W\n",
      "[codecarbon INFO @ 11:20:12] 0.025123 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:20:27] Energy consumed for RAM : 0.001455 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:20:27] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523723250468748 W\n",
      "[codecarbon INFO @ 11:20:27] Energy consumed for All CPU : 0.000766 kWh\n",
      "[codecarbon INFO @ 11:20:27] Energy consumed for all GPUs : 0.024401 kWh. Total GPU Power : 330.2936950472233 W\n",
      "[codecarbon INFO @ 11:20:27] 0.026622 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:20:42] Energy consumed for RAM : 0.001535 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:20:42] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524299617781251 W\n",
      "[codecarbon INFO @ 11:20:42] Energy consumed for All CPU : 0.000808 kWh\n",
      "[codecarbon INFO @ 11:20:42] Energy consumed for all GPUs : 0.025782 kWh. Total GPU Power : 331.56063485485834 W\n",
      "[codecarbon INFO @ 11:20:42] 0.028126 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:20:57] Energy consumed for RAM : 0.001616 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:20:57] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524523104375001 W\n",
      "[codecarbon INFO @ 11:20:57] Energy consumed for All CPU : 0.000850 kWh\n",
      "[codecarbon INFO @ 11:20:57] Energy consumed for all GPUs : 0.027165 kWh. Total GPU Power : 331.9020587339644 W\n",
      "[codecarbon INFO @ 11:20:57] 0.029631 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:21:12] Energy consumed for RAM : 0.001696 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:21:12] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524370959375005 W\n",
      "[codecarbon INFO @ 11:21:12] Energy consumed for All CPU : 0.000893 kWh\n",
      "[codecarbon INFO @ 11:21:12] Energy consumed for all GPUs : 0.028542 kWh. Total GPU Power : 330.526830859229 W\n",
      "[codecarbon INFO @ 11:21:12] 0.031131 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:21:27] Energy consumed for RAM : 0.001777 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:21:27] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524359654812502 W\n",
      "[codecarbon INFO @ 11:21:27] Energy consumed for All CPU : 0.000935 kWh\n",
      "[codecarbon INFO @ 11:21:27] Energy consumed for all GPUs : 0.029828 kWh. Total GPU Power : 308.6355627945435 W\n",
      "[codecarbon INFO @ 11:21:27] 0.032540 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:21:42] Energy consumed for RAM : 0.001857 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:21:42] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.529484549281253 W\n",
      "[codecarbon INFO @ 11:21:42] Energy consumed for All CPU : 0.000978 kWh\n",
      "[codecarbon INFO @ 11:21:42] Energy consumed for all GPUs : 0.031214 kWh. Total GPU Power : 332.8498902136568 W\n",
      "[codecarbon INFO @ 11:21:42] 0.034049 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:21:57] Energy consumed for RAM : 0.001938 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:21:57] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52460826659375 W\n",
      "[codecarbon INFO @ 11:21:57] Energy consumed for All CPU : 0.001020 kWh\n",
      "[codecarbon INFO @ 11:21:57] Energy consumed for all GPUs : 0.032593 kWh. Total GPU Power : 330.9759389053404 W\n",
      "[codecarbon INFO @ 11:21:57] 0.035551 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:21:57] 0.020405 g.CO2eq/s mean an estimation of 643.4996397480638 kg.CO2eq/year\n",
      "[codecarbon INFO @ 11:22:12] Energy consumed for RAM : 0.002019 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:22:12] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52344185309375 W\n",
      "[codecarbon INFO @ 11:22:12] Energy consumed for All CPU : 0.001062 kWh\n",
      "[codecarbon INFO @ 11:22:12] Energy consumed for all GPUs : 0.033985 kWh. Total GPU Power : 334.14457567278794 W\n",
      "[codecarbon INFO @ 11:22:12] 0.037066 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:22:27] Energy consumed for RAM : 0.002099 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:22:27] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523511103875 W\n",
      "[codecarbon INFO @ 11:22:27] Energy consumed for All CPU : 0.001105 kWh\n",
      "[codecarbon INFO @ 11:22:27] Energy consumed for all GPUs : 0.035377 kWh. Total GPU Power : 334.2077652407846 W\n",
      "[codecarbon INFO @ 11:22:27] 0.038581 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:22:42] Energy consumed for RAM : 0.002180 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:22:42] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52416338421875 W\n",
      "[codecarbon INFO @ 11:22:42] Energy consumed for All CPU : 0.001147 kWh\n",
      "[codecarbon INFO @ 11:22:42] Energy consumed for all GPUs : 0.036768 kWh. Total GPU Power : 334.0476946168456 W\n",
      "[codecarbon INFO @ 11:22:42] 0.040095 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:22:57] Energy consumed for RAM : 0.002260 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:22:57] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52357812209375 W\n",
      "[codecarbon INFO @ 11:22:57] Energy consumed for All CPU : 0.001189 kWh\n",
      "[codecarbon INFO @ 11:22:57] Energy consumed for all GPUs : 0.038157 kWh. Total GPU Power : 333.2397870038031 W\n",
      "[codecarbon INFO @ 11:22:57] 0.041606 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:23:12] Energy consumed for RAM : 0.002341 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:23:12] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523810574375 W\n",
      "[codecarbon INFO @ 11:23:12] Energy consumed for All CPU : 0.001232 kWh\n",
      "[codecarbon INFO @ 11:23:12] Energy consumed for all GPUs : 0.039495 kWh. Total GPU Power : 320.57051404896623 W\n",
      "[codecarbon INFO @ 11:23:12] 0.043068 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:23:27] Energy consumed for RAM : 0.002421 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:23:27] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52783437846875 W\n",
      "[codecarbon INFO @ 11:23:27] Energy consumed for All CPU : 0.001274 kWh\n",
      "[codecarbon INFO @ 11:23:27] Energy consumed for all GPUs : 0.040839 kWh. Total GPU Power : 323.0159276754214 W\n",
      "[codecarbon INFO @ 11:23:27] 0.044534 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:23:42] Energy consumed for RAM : 0.002502 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:23:42] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523729841843751 W\n",
      "[codecarbon INFO @ 11:23:42] Energy consumed for All CPU : 0.001317 kWh\n",
      "[codecarbon INFO @ 11:23:42] Energy consumed for all GPUs : 0.042240 kWh. Total GPU Power : 336.4582987309185 W\n",
      "[codecarbon INFO @ 11:23:42] 0.046059 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:23:57] Energy consumed for RAM : 0.002582 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:23:57] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52315159634375 W\n",
      "[codecarbon INFO @ 11:23:57] Energy consumed for All CPU : 0.001359 kWh\n",
      "[codecarbon INFO @ 11:23:57] Energy consumed for all GPUs : 0.043635 kWh. Total GPU Power : 334.9149526464775 W\n",
      "[codecarbon INFO @ 11:23:57] 0.047577 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:23:57] 0.020569 g.CO2eq/s mean an estimation of 648.6724457807306 kg.CO2eq/year\n",
      "[codecarbon INFO @ 11:24:12] Energy consumed for RAM : 0.002663 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:24:12] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524091759125001 W\n",
      "[codecarbon INFO @ 11:24:12] Energy consumed for All CPU : 0.001401 kWh\n",
      "[codecarbon INFO @ 11:24:12] Energy consumed for all GPUs : 0.045025 kWh. Total GPU Power : 333.6276445760722 W\n",
      "[codecarbon INFO @ 11:24:12] 0.049089 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:24:27] Energy consumed for RAM : 0.002743 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:24:27] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.523868378843751 W\n",
      "[codecarbon INFO @ 11:24:27] Energy consumed for All CPU : 0.001444 kWh\n",
      "[codecarbon INFO @ 11:24:27] Energy consumed for all GPUs : 0.046417 kWh. Total GPU Power : 334.05621042796207 W\n",
      "[codecarbon INFO @ 11:24:27] 0.050604 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:24:42] Energy consumed for RAM : 0.002824 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:24:42] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5243619936875 W\n",
      "[codecarbon INFO @ 11:24:42] Energy consumed for All CPU : 0.001486 kWh\n",
      "[codecarbon INFO @ 11:24:42] Energy consumed for all GPUs : 0.047805 kWh. Total GPU Power : 333.1718896604736 W\n",
      "[codecarbon INFO @ 11:24:42] 0.052115 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:24:57] Energy consumed for RAM : 0.002904 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:24:57] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5240761311875 W\n",
      "[codecarbon INFO @ 11:24:57] Energy consumed for All CPU : 0.001528 kWh\n",
      "[codecarbon INFO @ 11:24:57] Energy consumed for all GPUs : 0.049205 kWh. Total GPU Power : 336.15868887443696 W\n",
      "[codecarbon INFO @ 11:24:57] 0.053638 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 11:25:04] Energy consumed for RAM : 0.002943 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 11:25:05] Delta energy consumed for CPU with cpu_load : 0.000021 kWh, power : 10.515858801 W\n",
      "[codecarbon INFO @ 11:25:05] Energy consumed for All CPU : 0.001549 kWh\n",
      "[codecarbon INFO @ 11:25:05] Energy consumed for all GPUs : 0.049770 kWh. Total GPU Power : 269.82514436971525 W\n",
      "[codecarbon INFO @ 11:25:05] 0.054262 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=2.330583740234375, metrics={'train_runtime': 548.4807, 'train_samples_per_second': 7.293, 'train_steps_per_second': 0.912, 'total_flos': 4397852000256000.0, 'train_loss': 2.330583740234375, 'epoch': 1.6666666666666665})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "# Data collator for causal LM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # causal LM\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qwen-sft-final',\n",
    "    per_device_train_batch_size=1,  # small batch for stability\n",
    "    gradient_accumulation_steps=4,  # effective batch size = 4\n",
    "    learning_rate=1e-5,             # start small\n",
    "    max_steps=500,                   # short debug run\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    report_to=None,                  # no wandb/other reporting\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,     # save memory\n",
    "    max_grad_norm=1.0,               # gradient clipping\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "412a6949-d3c5-406c-8bc6-aa2463a361da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.trainer.Trainer'>\n",
      "<class 'transformers.training_args.TrainingArguments'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "print(Trainer)\n",
    "print(TrainingArguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7e875d-7f85-46e7-a743-b054ac11e339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen-sft-final/tokenizer_config.json',\n",
       " './qwen-sft-final/special_tokens_map.json',\n",
       " './qwen-sft-final/chat_template.jinja',\n",
       " './qwen-sft-final/vocab.json',\n",
       " './qwen-sft-final/merges.txt',\n",
       " './qwen-sft-final/added_tokens.json',\n",
       " './qwen-sft-final/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./qwen-sft-final\")\n",
    "tokenizer.save_pretrained(\"./qwen-sft-final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fbeba2c-b5e0-42ca-a6ad-71cb10eaf0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 25140, 7822, 5244, 389, 16052, 15712, 323, 10601, 51212, 369, 20622, 9387, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "51212\n",
      "151643\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"][0][\"labels\"][:50])\n",
    "print(max(tokenized_dataset[\"train\"][0][\"labels\"]))\n",
    "print(tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c052841-26f4-4180-9cdf-896d3892408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad labels: []\n"
     ]
    }
   ],
   "source": [
    "def check_labels(dataset, tokenizer):\n",
    "    bad_batches = []\n",
    "    for i, batch in enumerate(dataset):\n",
    "        labels = batch[\"labels\"]\n",
    "        for l in labels:\n",
    "            if (l != -100 and (l < 0 or l >= tokenizer.vocab_size)):\n",
    "                bad_batches.append((i, l, tokenizer.vocab_size))\n",
    "    return bad_batches\n",
    "\n",
    "bad = check_labels(tokenized_dataset[\"train\"], tokenizer)\n",
    "print(\"Bad labels:\", bad[:20])  # show first 20 errors if any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1311787c-127a-4790-a795-8e86aa425d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")  # replaces load_metric(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels with pad token ID before decoding\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # BLEU expects list of references per prediction\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "    \n",
    "    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0b7864-123f-405e-9224-ab69ca5e900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/anaconda3/envs/qwen32b/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': nan,\n",
       " 'eval_runtime': 8.0001,\n",
       " 'eval_samples_per_second': 37.499,\n",
       " 'eval_steps_per_second': 2.375,\n",
       " 'epoch': 1.6666666666666665}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_dataset[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c6435-319b-408e-aeae-96c6c44fd2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280e649-545e-4e14-a537-c60a19860ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943cec3-fd6b-47ba-89e4-451ec658145d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen32b)",
   "language": "python",
   "name": "qwen32b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
