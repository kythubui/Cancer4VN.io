{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb134fd0-d917-480b-9080-5d3c5b51a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.cuda.get_device_name(0))  # GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c093eb4-bfdc-4def-b9da-a6750c300e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0deb7180-9d17-44c7-978a-013869b3b3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the survival rate for lung cancer pati...</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is lung cancer diagnosed?</td>\n",
       "      <td>Air pollution, particularly fine particulate m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is lung cancer hereditary?</td>\n",
       "      <td>Air pollution, particularly fine particulate m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the stages of lung cancer?</td>\n",
       "      <td>Targeted therapies focus on specific genetic m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are there clinical trials for lung cancer trea...</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>What treatments are available for Stage 1 lung...</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Are there alternative therapies for lung cancer?</td>\n",
       "      <td>The survival rate depends on the stage at diag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Are there clinical trials for lung cancer trea...</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>How is lung cancer diagnosed?</td>\n",
       "      <td>Some alternative therapies like acupuncture ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>How is lung cancer diagnosed?</td>\n",
       "      <td>Numerous clinical trials are exploring targete...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  \\\n",
       "0     What is the survival rate for lung cancer pati...   \n",
       "1                         How is lung cancer diagnosed?   \n",
       "2                            Is lung cancer hereditary?   \n",
       "3                   What are the stages of lung cancer?   \n",
       "4     Are there clinical trials for lung cancer trea...   \n",
       "...                                                 ...   \n",
       "2995  What treatments are available for Stage 1 lung...   \n",
       "2996   Are there alternative therapies for lung cancer?   \n",
       "2997  Are there clinical trials for lung cancer trea...   \n",
       "2998                      How is lung cancer diagnosed?   \n",
       "2999                      How is lung cancer diagnosed?   \n",
       "\n",
       "                                                 output  \n",
       "0     Numerous clinical trials are exploring targete...  \n",
       "1     Air pollution, particularly fine particulate m...  \n",
       "2     Air pollution, particularly fine particulate m...  \n",
       "3     Targeted therapies focus on specific genetic m...  \n",
       "4     Numerous clinical trials are exploring targete...  \n",
       "...                                                 ...  \n",
       "2995  Numerous clinical trials are exploring targete...  \n",
       "2996  The survival rate depends on the stage at diag...  \n",
       "2997  Numerous clinical trials are exploring targete...  \n",
       "2998  Some alternative therapies like acupuncture ma...  \n",
       "2999  Numerous clinical trials are exploring targete...  \n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hf://datasets/SandeepKumarRudhravaram/Lung_Cancer_QA/Regenerated_Lung_Cancer_QA_Dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0c3541-85a3-4d9e-a541-6256fbd78dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (2400, 2)\n",
      "Shape of validation set: (300, 2)\n",
      "Shape of test set: (300, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Shape of training set: {train_df.shape}\")\n",
    "print(f\"Shape of validation set: {validation_df.shape}\")\n",
    "print(f\"Shape of test set: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b853c59a-27bf-4bbc-a8cd-f96e80c2bd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nimport pandas as pd\\nfrom tqdm import tqdm\\n\\n# Load model + tokenizer\\nmodel_id = \"openai/gpt-oss-20b\"\\n#model_id = \"openai/gpt-oss-6.9b\"\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    torch_dtype=torch.float32  \\n)\\n\\n# Assume test_df has a column \"prompt\"\\nprompts = test_df[\"prompt\"].tolist()\\n\\noutputs = []\\nbatch_size = 2   # adjust depending on your GPU memory\\n\\nfor i in tqdm(range(0, len(prompts), batch_size)):\\n    batch_prompts = prompts[i:i+batch_size]\\n    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\\n    \\n    out_tokens = model.generate(\\n        **inputs,\\n        max_new_tokens=256,\\n        temperature=0.7,\\n        do_sample=True,\\n        pad_token_id=tokenizer.eos_token_id,\\n    )\\n    \\n    decoded_batch = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\\n    outputs.extend(decoded_batch)\\n\\n# Add results to DataFrame\\ntest_df[\"model_output\"] = outputs\\ntest_df.to_csv(\"test_with_outputs.csv\", index=False)\\n\\nprint(\"✅ Finished inference. Results saved to test_with_outputs.csv\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model + tokenizer\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "#model_id = \"openai/gpt-oss-6.9b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32  \n",
    ")\n",
    "\n",
    "# Assume test_df has a column \"prompt\"\n",
    "prompts = test_df[\"prompt\"].tolist()\n",
    "\n",
    "outputs = []\n",
    "batch_size = 2   # adjust depending on your GPU memory\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    out_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    decoded_batch = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\n",
    "    outputs.extend(decoded_batch)\n",
    "\n",
    "# Add results to DataFrame\n",
    "test_df[\"model_output\"] = outputs\n",
    "test_df.to_csv(\"test_with_outputs.csv\", index=False)\n",
    "\n",
    "print(\"✅ Finished inference. Results saved to test_with_outputs.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deaa7a6b-dfa7-4d15-a1f9-77cc8c52d9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nimport pandas as pd\\nfrom tqdm import tqdm\\n\\n# Load model + tokenizer\\nmodel_id = \"openai/gpt-oss-20b\"\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    torch_dtype=torch.float32,   # use float32 if CPU only\\n)\\n\\n# Assume test_df has a column \"prompt\"\\nprompts = test_df[\"prompt\"].tolist()\\n\\noutputs = []\\nbatch_size = 2   # adjust depending on your GPU memory\\n\\nfor i in tqdm(range(0, len(prompts), batch_size)):\\n    batch_prompts = prompts[i:i+batch_size]\\n    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\\n    \\n    out_tokens = model.generate(\\n        **inputs,\\n        max_new_tokens=256,\\n        temperature=0.7,\\n        do_sample=True,\\n        pad_token_id=tokenizer.eos_token_id,\\n    )\\n    \\n    decoded_batch = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\\n    outputs.extend(decoded_batch)\\n\\n# Add results to DataFrame\\ntest_df[\"model_output\"] = outputs\\ntest_df.to_csv(\"test_with_outputs.csv\", index=False)\\n\\nprint(\"✅ Finished inference. Results saved to test_with_outputs.csv\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model + tokenizer\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,   # use float32 if CPU only\n",
    ")\n",
    "\n",
    "# Assume test_df has a column \"prompt\"\n",
    "prompts = test_df[\"prompt\"].tolist()\n",
    "\n",
    "outputs = []\n",
    "batch_size = 2   # adjust depending on your GPU memory\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    out_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    decoded_batch = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\n",
    "    outputs.extend(decoded_batch)\n",
    "\n",
    "# Add results to DataFrame\n",
    "test_df[\"model_output\"] = outputs\n",
    "test_df.to_csv(\"test_with_outputs.csv\", index=False)\n",
    "\n",
    "print(\"✅ Finished inference. Results saved to test_with_outputs.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99296b6f-a0f9-4c51-bf42-194676b81aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\\nimport pandas as pd\\nfrom tqdm import tqdm\\n\\n# Load your test dataset\\n# test_df = pd.read_csv(\"your_test_file.csv\")  # make sure test_df has a column \"prompt\"\\n\\n# Model ID\\nmodel_id = \"EleutherAI/gpt-j-6B\"\\n\\n# Initialize tokenizer and model pipeline\\ngenerator = pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    device=-1  # -1 means CPU\\n)\\n\\nprompts = test_df[\"input\"].tolist()\\noutputs = []\\n\\n# Generate outputs for each prompt\\nfor prompt in tqdm(prompts):\\n    result = generator(\\n        prompt,\\n        max_length=256,   # adjust as needed\\n        do_sample=True,\\n        temperature=0.7\\n    )\\n    outputs.append(result[0][\\'generated_text\\'])\\n\\n# Save results to DataFrame\\ntest_df[\"model_output\"] = outputs\\ntest_df.to_csv(\"test_with_outputs.csv\", index=False)\\nprint(\"✅ Finished inference, results saved to test_with_outputs.csv\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your test dataset\n",
    "# test_df = pd.read_csv(\"your_test_file.csv\")  # make sure test_df has a column \"prompt\"\n",
    "\n",
    "# Model ID\n",
    "model_id = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "# Initialize tokenizer and model pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device=-1  # -1 means CPU\n",
    ")\n",
    "\n",
    "prompts = test_df[\"input\"].tolist()\n",
    "outputs = []\n",
    "\n",
    "# Generate outputs for each prompt\n",
    "for prompt in tqdm(prompts):\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=256,   # adjust as needed\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    outputs.append(result[0]['generated_text'])\n",
    "\n",
    "# Save results to DataFrame\n",
    "test_df[\"model_output\"] = outputs\n",
    "test_df.to_csv(\"test_with_outputs.csv\", index=False)\n",
    "print(\"✅ Finished inference, results saved to test_with_outputs.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d72a2b2-c132-4bc5-aa10-ea65a3b4a359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|███████████████████████████████████████████| 38/38 [11:35<00:00, 18.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished inference on GPU using accelerate. Results saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "model_id = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "# Load tokenizer + model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # FP16 for GPU\n",
    "    device_map=\"auto\"           # let accelerate handle GPU placement\n",
    ")\n",
    "\n",
    "# Do NOT set device here — the model already knows where to go\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Batched inference\n",
    "prompts = test_df[\"input\"].tolist()\n",
    "outputs = []\n",
    "batch_size = 8\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    results = generator(\n",
    "        batch_prompts,\n",
    "        max_new_tokens=128,\n",
    "        truncation=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # results is a list of lists, one list per prompt\n",
    "    for r in results:\n",
    "        outputs.append(r[0][\"generated_text\"])\n",
    "\n",
    "# Save outputs\n",
    "test_df[\"model_output\"] = outputs\n",
    "test_df.to_csv(\"test_with_qwen_outputs.csv\", index=False)\n",
    "print(\"✅ Finished inference on GPU using accelerate. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e66ee2-41f6-43cb-98fc-f98273e7183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.29\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "test_df = pd.read_csv(\"test_with_qwen_outputs.csv\")\n",
    "\n",
    "references = [[ref.split()] for ref in test_df[\"output\"].tolist()]  # each ref must be a list of tokens\n",
    "hypotheses = [hyp.split() for hyp in test_df[\"model_output\"].tolist()]\n",
    "\n",
    "# Smoothing function helps with short sentences\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "# Compute corpus BLEU\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "print(f\"BLEU score: {bleu_score*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94dfd6-38fd-472f-a901-d43b56057c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87af5dc0-5042-4323-8915-7166395d2580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39130786-98df-4b87-b7f3-6c9afb2c9b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
