{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9d7582-2924-467e-90a3-24c8b61d3fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.cuda.get_device_name(0))  # GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901649d4-6c5b-4c06-975f-286a3cde04cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 14:51:27.563632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-30 14:51:27.563658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-30 14:51:27.564433: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-30 14:51:27.568251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-30 14:51:28.115364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7be530-a9ac-49ab-bcbe-7dab8851414f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36800818</td>\n",
       "      <td>Evaluating the effect of nurses supportive and...</td>\n",
       "      <td>Endometriosis is one of the womens aggressive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36800748</td>\n",
       "      <td>18F-FDG PET-CT in Cutaneous Metastases in Brea...</td>\n",
       "      <td>Cutaneous and subcutaneous metastases are an e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36800716</td>\n",
       "      <td>Efficacy of pre - operative axillary ultrasono...</td>\n",
       "      <td>To determine false negative rate, negative pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36800710</td>\n",
       "      <td>Is complete pathological response truly a comp...</td>\n",
       "      <td>To check if complete pathological response in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36800640</td>\n",
       "      <td>Experiences of a Multiethnic Cohort of Patient...</td>\n",
       "      <td>Financial reimbursement programs (FRPs) offset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9508</th>\n",
       "      <td>36194067</td>\n",
       "      <td>The Lumbar Artery Perforator Flap in Breast Re...</td>\n",
       "      <td>The lumbar artery perforator flap is a valuabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9509</th>\n",
       "      <td>36194066</td>\n",
       "      <td>Sensory Restoration in Abdominally Based Free ...</td>\n",
       "      <td>Neurotization in breast reconstruction can be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9510</th>\n",
       "      <td>31915335</td>\n",
       "      <td>StatPearls</td>\n",
       "      <td>Breast lumps or masses are very common, partic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9511</th>\n",
       "      <td>26389406</td>\n",
       "      <td>PDQ Cancer Information Summaries</td>\n",
       "      <td>This PDQ cancer information summary has curren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9512</th>\n",
       "      <td>26389160</td>\n",
       "      <td>PDQ Cancer Information Summaries</td>\n",
       "      <td>This PDQ cancer information summary has curren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9513 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pubmed_id                                              title  \\\n",
       "0      36800818  Evaluating the effect of nurses supportive and...   \n",
       "1      36800748  18F-FDG PET-CT in Cutaneous Metastases in Brea...   \n",
       "2      36800716  Efficacy of pre - operative axillary ultrasono...   \n",
       "3      36800710  Is complete pathological response truly a comp...   \n",
       "4      36800640  Experiences of a Multiethnic Cohort of Patient...   \n",
       "...         ...                                                ...   \n",
       "9508   36194067  The Lumbar Artery Perforator Flap in Breast Re...   \n",
       "9509   36194066  Sensory Restoration in Abdominally Based Free ...   \n",
       "9510   31915335                                         StatPearls   \n",
       "9511   26389406                   PDQ Cancer Information Summaries   \n",
       "9512   26389160                   PDQ Cancer Information Summaries   \n",
       "\n",
       "                                               abstract  \n",
       "0     Endometriosis is one of the womens aggressive ...  \n",
       "1     Cutaneous and subcutaneous metastases are an e...  \n",
       "2     To determine false negative rate, negative pre...  \n",
       "3     To check if complete pathological response in ...  \n",
       "4     Financial reimbursement programs (FRPs) offset...  \n",
       "...                                                 ...  \n",
       "9508  The lumbar artery perforator flap is a valuabl...  \n",
       "9509  Neurotization in breast reconstruction can be ...  \n",
       "9510  Breast lumps or masses are very common, partic...  \n",
       "9511  This PDQ cancer information summary has curren...  \n",
       "9512  This PDQ cancer information summary has curren...  \n",
       "\n",
       "[9513 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hf://datasets/Gaborandi/breast_cancer_pubmed_abstracts/breast_cancer.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5a0d65-aceb-42a3-85a1-9baad6fff010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (7610, 3)\n",
      "Shape of validation set: (951, 3)\n",
      "Shape of test set: (952, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Shape of training set: {train_df.shape}\")\n",
    "print(f\"Shape of validation set: {validation_df.shape}\")\n",
    "print(f\"Shape of test set: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8025a621-85b1-4ed6-ba76-a21531fb7c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa58362714f4958a40488a313aaceda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b545b8d-73be-48c9-99c2-2cf3fad3dfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# === Qwen3-32B 4-bit GPU setup and generation ===\\n\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\\nimport torch\\n\\n# --- Step 1: BitsAndBytes 4-bit config ---\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=False,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.bfloat16,\\n)\\n\\n# --- Step 2: Load tokenizer ---\\nmodel_dir = \"Qwen/Qwen3-32B\"\\ntokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\\n\\n# --- Step 3: Load model on GPU(s) ---\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_dir,\\n    quantization_config=bnb_config,\\n    device_map=\"auto\",       # Automatically distributes layers across GPUs\\n    torch_dtype=torch.bfloat16,\\n    trust_remote_code=True,\\n    offload_folder=\"offload\",  # Optional: offload layers to CPU if GPU memory is low\\n    offload_state_dict=True\\n)\\n\\n# Disable caching to save GPU memory\\nmodel.config.use_cache = False\\nmodel.config.pretraining_tp = 1\\n\\n# --- Step 4: Check GPU devices ---\\nprint(\"CUDA available:\", torch.cuda.is_available())\\nprint(\"Number of GPUs:\", torch.cuda.device_count())\\nprint(\"Model device map:\", model.hf_device_map)\\n\\n# --- Step 5: Generate text ---\\nprompt = \"Once upon a time in a futuristic city,\"\\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move inputs to GPU\\n\\n# Generate output\\nwith torch.no_grad():\\n    outputs = model.generate(\\n        **inputs,\\n        max_new_tokens=100,\\n        do_sample=True,\\n        top_p=0.95,\\n        temperature=0.8\\n    )\\n\\n# Decode and print\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(\"=== Generated Text ===\")\\nprint(generated_text)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# === Qwen3-32B 4-bit GPU setup and generation ===\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# --- Step 1: BitsAndBytes 4-bit config ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- Step 2: Load tokenizer ---\n",
    "model_dir = \"Qwen/Qwen3-32B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "# --- Step 3: Load model on GPU(s) ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",       # Automatically distributes layers across GPUs\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    offload_folder=\"offload\",  # Optional: offload layers to CPU if GPU memory is low\n",
    "    offload_state_dict=True\n",
    ")\n",
    "\n",
    "# Disable caching to save GPU memory\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# --- Step 4: Check GPU devices ---\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Model device map:\", model.hf_device_map)\n",
    "\n",
    "# --- Step 5: Generate text ---\n",
    "prompt = \"Once upon a time in a futuristic city,\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move inputs to GPU\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Generated Text ===\")\n",
    "print(generated_text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08c679d-94bb-4dee-904f-7f922b5a1551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Text ===\n",
      "Once upon a time in a futuristic city, the only way to get around was the bike. Every vehicle must be painted black and black is a very popular color. As a result, the city was nicknamed \"Black City.\" Black cars and bikes were used in a strange way to solve problems. One day, the city's people were attacked by a band of aliens, and the city was put under a state of emergency.\n",
      "After the aliens have destroyed the city, the city's inhabitants and their leaders want to build a new city. The city\n",
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "Model device map: {'': 1}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# --- Step 1: 4-bit config for VRAM efficiency ---\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# --- Step 2: Load tokenizer ---\n",
    "model_dir = \"Qwen/Qwen2-0.5B\"  # Choose the 0.5B model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "# --- Step 3: Load model across multiple GPUs ---\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",       # Automatically shard across all available GPUs\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Disable caching to save VRAM\n",
    "model.config.use_cache = False\n",
    "\n",
    "# --- Step 4: Determine first device for inputs ---\n",
    "first_device = list(model.hf_device_map.values())[0]\n",
    "\n",
    "# --- Step 5: Generate text ---\n",
    "prompt = \"Once upon a time in a futuristic city,\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(first_device)  # Place on first shard\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"=== Generated Text ===\")\n",
    "print(generated_text)\n",
    "\n",
    "# --- Step 6: Optional info ---\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "print(\"Model device map:\", model.hf_device_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f609fab3-6675-4484-acf1-254ced98c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_qa(row):\n",
    "    return f\"Question: {row['title']}\\nAnswer: {row['abstract']}\\n\"\n",
    "\n",
    "train_texts = train_df.apply(combine_qa, axis=1).tolist()\n",
    "val_texts = validation_df.apply(combine_qa, axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "792f8992-6e94-4dd1-9723-da3f053ab4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings = tokenize_function(val_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e99d79-60f0-4158-8ef6-82df586de2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",        # Multi-GPU\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ab417f8-2182-4c38-b850-71343098a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "        self.attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx]),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask[idx]),\n",
    "            \"labels\": torch.tensor(self.input_ids[idx])\n",
    "        }\n",
    "\n",
    "train_dataset = QADataset(train_encodings)\n",
    "val_dataset = QADataset(val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6191172-f8a7-4755-8f12-bdd65e37a702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=False,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.bfloat16,\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    quantization_config=bnb_config,\\n    device_map=\"auto\",  # Automatically shard across GPUs\\n    torch_dtype=torch.bfloat16,\\n    trust_remote_code=True\\n)\\n\\nmodel.config.use_cache = False\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically shard across GPUs\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10df7a40-811f-4e61-befd-137e6dcc9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")  # your GPU\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": device},  # pin model to single GPU\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40f38454-50f1-47bf-a793-612d3f47da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # adjust if needed\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b3fba72-b92c-4715-a9c6-ecf2ddb666cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen2-0.5B-finetuned-breastcancer\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_total_limit=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2de6697d-d79d-47d1-8df2-a8e9ffcbfc5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 14:52:30] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 14:52:30] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:52:30] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:52:31] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 14:52:31] CPU Model on constant consumption mode: AMD Ryzen 7 5800X 8-Core Processor\n",
      "[codecarbon WARNING @ 14:52:31] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 14:52:31] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:52:31] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 14:52:31] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 14:52:31] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:52:31]   Platform system: Linux-5.15.0-153-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 14:52:31]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 14:52:31]   CodeCarbon version: 3.0.5\n",
      "[codecarbon INFO @ 14:52:31]   Available RAM : 62.701 GB\n",
      "[codecarbon INFO @ 14:52:31]   CPU count: 16 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 14:52:31]   CPU model: AMD Ryzen 7 5800X 8-Core Processor\n",
      "[codecarbon INFO @ 14:52:31]   GPU count: 2\n",
      "[codecarbon INFO @ 14:52:31]   GPU model: 2 x NVIDIA GeForce RTX 3090\n",
      "[codecarbon INFO @ 14:52:35] Emissions data (if any) will be saved to file /home/bobby/Cancer4VN/qwen2-0.5B-finetuned-breastcancer/emissions.csv\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f5c1e8-deb4-47e9-b159-e3a6665b2620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobbyhieubui\u001b[0m (\u001b[33mbobbyhieubui-university-of-wisconsin-madison\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bobby/Cancer4VN/wandb/run-20250930_145236-kxcf8cjq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/kxcf8cjq' target=\"_blank\">whole-cosmos-6</a></strong> to <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface' target=\"_blank\">https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/kxcf8cjq' target=\"_blank\">https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/kxcf8cjq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1428' max='1428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1428/1428 12:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.782500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.750500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.738700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.713500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.715800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.783600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.736500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.769100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.750800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.736800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.726600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.757300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.754700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.723800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.747600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.731900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.757300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.731300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.759600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.723200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.739600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.725600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.712100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.706600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.713500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.710500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.711500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.716100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.655600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.704200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.715700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.718500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.746800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.733800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.702300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.766500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.740500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.761300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.731800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.731100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.701800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.731400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.660100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:52:52] Energy consumed for RAM : 0.000086 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:52:53] Delta energy consumed for CPU with cpu_load : 0.000045 kWh, power : 10.5246044547 W\n",
      "[codecarbon INFO @ 14:52:53] Energy consumed for All CPU : 0.000045 kWh\n",
      "[codecarbon INFO @ 14:52:53] Energy consumed for all GPUs : 0.001296 kWh. Total GPU Power : 291.3482739124505 W\n",
      "[codecarbon INFO @ 14:52:53] 0.001427 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:53:07] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:53:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524781603218754 W\n",
      "[codecarbon INFO @ 14:53:08] Energy consumed for All CPU : 0.000088 kWh\n",
      "[codecarbon INFO @ 14:53:08] Energy consumed for all GPUs : 0.002585 kWh. Total GPU Power : 309.54667849815144 W\n",
      "[codecarbon INFO @ 14:53:08] 0.002840 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:53:22] Energy consumed for RAM : 0.000247 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:53:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525136255812502 W\n",
      "[codecarbon INFO @ 14:53:23] Energy consumed for All CPU : 0.000130 kWh\n",
      "[codecarbon INFO @ 14:53:23] Energy consumed for all GPUs : 0.003867 kWh. Total GPU Power : 307.8109698729223 W\n",
      "[codecarbon INFO @ 14:53:23] 0.004244 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:53:37] Energy consumed for RAM : 0.000328 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:53:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52458317684375 W\n",
      "[codecarbon INFO @ 14:53:38] Energy consumed for All CPU : 0.000172 kWh\n",
      "[codecarbon INFO @ 14:53:38] Energy consumed for all GPUs : 0.005165 kWh. Total GPU Power : 311.5676657240688 W\n",
      "[codecarbon INFO @ 14:53:38] 0.005665 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:53:52] Energy consumed for RAM : 0.000408 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:53:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52550749315625 W\n",
      "[codecarbon INFO @ 14:53:53] Energy consumed for All CPU : 0.000215 kWh\n",
      "[codecarbon INFO @ 14:53:53] Energy consumed for all GPUs : 0.006465 kWh. Total GPU Power : 312.01128512138473 W\n",
      "[codecarbon INFO @ 14:53:53] 0.007088 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:54:07] Energy consumed for RAM : 0.000489 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:54:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.526445458812502 W\n",
      "[codecarbon INFO @ 14:54:08] Energy consumed for All CPU : 0.000257 kWh\n",
      "[codecarbon INFO @ 14:54:08] Energy consumed for all GPUs : 0.007774 kWh. Total GPU Power : 314.4038030769729 W\n",
      "[codecarbon INFO @ 14:54:08] 0.008520 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:54:22] Energy consumed for RAM : 0.000569 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:54:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525984121625 W\n",
      "[codecarbon INFO @ 14:54:23] Energy consumed for All CPU : 0.000300 kWh\n",
      "[codecarbon INFO @ 14:54:23] Energy consumed for all GPUs : 0.009097 kWh. Total GPU Power : 317.5847031146786 W\n",
      "[codecarbon INFO @ 14:54:23] 0.009966 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:54:37] Energy consumed for RAM : 0.000650 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:54:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52551213546875 W\n",
      "[codecarbon INFO @ 14:54:38] Energy consumed for All CPU : 0.000342 kWh\n",
      "[codecarbon INFO @ 14:54:38] Energy consumed for all GPUs : 0.010434 kWh. Total GPU Power : 320.8686469756259 W\n",
      "[codecarbon INFO @ 14:54:38] 0.011426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:54:38] 0.019379 g.CO2eq/s mean an estimation of 611.1451023084883 kg.CO2eq/year\n",
      "[codecarbon INFO @ 14:54:52] Energy consumed for RAM : 0.000730 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:54:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5251106699375 W\n",
      "[codecarbon INFO @ 14:54:53] Energy consumed for All CPU : 0.000384 kWh\n",
      "[codecarbon INFO @ 14:54:53] Energy consumed for all GPUs : 0.011778 kWh. Total GPU Power : 322.7627751649037 W\n",
      "[codecarbon INFO @ 14:54:53] 0.012893 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:55:07] Energy consumed for RAM : 0.000811 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:55:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5248204840625 W\n",
      "[codecarbon INFO @ 14:55:08] Energy consumed for All CPU : 0.000427 kWh\n",
      "[codecarbon INFO @ 14:55:08] Energy consumed for all GPUs : 0.013132 kWh. Total GPU Power : 325.1881890949517 W\n",
      "[codecarbon INFO @ 14:55:08] 0.014370 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:55:22] Energy consumed for RAM : 0.000891 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:55:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5255862411875 W\n",
      "[codecarbon INFO @ 14:55:23] Energy consumed for All CPU : 0.000469 kWh\n",
      "[codecarbon INFO @ 14:55:23] Energy consumed for all GPUs : 0.014488 kWh. Total GPU Power : 325.30276933194716 W\n",
      "[codecarbon INFO @ 14:55:23] 0.015848 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:55:37] Energy consumed for RAM : 0.000972 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:55:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5255761415 W\n",
      "[codecarbon INFO @ 14:55:38] Energy consumed for All CPU : 0.000511 kWh\n",
      "[codecarbon INFO @ 14:55:38] Energy consumed for all GPUs : 0.015851 kWh. Total GPU Power : 327.1751290941755 W\n",
      "[codecarbon INFO @ 14:55:38] 0.017334 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:55:52] Energy consumed for RAM : 0.001052 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:55:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5258855699375 W\n",
      "[codecarbon INFO @ 14:55:53] Energy consumed for All CPU : 0.000554 kWh\n",
      "[codecarbon INFO @ 14:55:53] Energy consumed for all GPUs : 0.017221 kWh. Total GPU Power : 328.9429215952756 W\n",
      "[codecarbon INFO @ 14:55:53] 0.018827 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:56:07] Energy consumed for RAM : 0.001133 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:56:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525390449000001 W\n",
      "[codecarbon INFO @ 14:56:08] Energy consumed for All CPU : 0.000596 kWh\n",
      "[codecarbon INFO @ 14:56:08] Energy consumed for all GPUs : 0.018587 kWh. Total GPU Power : 327.9062006595668 W\n",
      "[codecarbon INFO @ 14:56:08] 0.020316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:56:22] Energy consumed for RAM : 0.001213 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:56:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.527157764375003 W\n",
      "[codecarbon INFO @ 14:56:23] Energy consumed for All CPU : 0.000639 kWh\n",
      "[codecarbon INFO @ 14:56:23] Energy consumed for all GPUs : 0.019961 kWh. Total GPU Power : 329.93606120097434 W\n",
      "[codecarbon INFO @ 14:56:23] 0.021813 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:56:37] Energy consumed for RAM : 0.001294 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:56:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525861608281252 W\n",
      "[codecarbon INFO @ 14:56:38] Energy consumed for All CPU : 0.000681 kWh\n",
      "[codecarbon INFO @ 14:56:38] Energy consumed for all GPUs : 0.021341 kWh. Total GPU Power : 331.22032110220323 W\n",
      "[codecarbon INFO @ 14:56:38] 0.023316 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:56:38] 0.020339 g.CO2eq/s mean an estimation of 641.3966874292521 kg.CO2eq/year\n",
      "[codecarbon INFO @ 14:56:52] Energy consumed for RAM : 0.001374 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:56:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.526353835156252 W\n",
      "[codecarbon INFO @ 14:56:53] Energy consumed for All CPU : 0.000723 kWh\n",
      "[codecarbon INFO @ 14:56:53] Energy consumed for all GPUs : 0.022720 kWh. Total GPU Power : 330.97232430664366 W\n",
      "[codecarbon INFO @ 14:56:53] 0.024818 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:57:07] Energy consumed for RAM : 0.001455 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:57:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52437481615625 W\n",
      "[codecarbon INFO @ 14:57:08] Energy consumed for All CPU : 0.000766 kWh\n",
      "[codecarbon INFO @ 14:57:08] Energy consumed for all GPUs : 0.024091 kWh. Total GPU Power : 329.1268370813895 W\n",
      "[codecarbon INFO @ 14:57:08] 0.026311 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:57:22] Energy consumed for RAM : 0.001535 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:57:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52507232065625 W\n",
      "[codecarbon INFO @ 14:57:23] Energy consumed for All CPU : 0.000808 kWh\n",
      "[codecarbon INFO @ 14:57:23] Energy consumed for all GPUs : 0.025482 kWh. Total GPU Power : 333.9652826294049 W\n",
      "[codecarbon INFO @ 14:57:23] 0.027825 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:57:37] Energy consumed for RAM : 0.001616 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:57:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524536712375001 W\n",
      "[codecarbon INFO @ 14:57:38] Energy consumed for All CPU : 0.000850 kWh\n",
      "[codecarbon INFO @ 14:57:38] Energy consumed for all GPUs : 0.026880 kWh. Total GPU Power : 335.5858095353895 W\n",
      "[codecarbon INFO @ 14:57:38] 0.029346 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:57:52] Energy consumed for RAM : 0.001696 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:57:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524513961500002 W\n",
      "[codecarbon INFO @ 14:57:53] Energy consumed for All CPU : 0.000893 kWh\n",
      "[codecarbon INFO @ 14:57:53] Energy consumed for all GPUs : 0.028274 kWh. Total GPU Power : 334.92327949319133 W\n",
      "[codecarbon INFO @ 14:57:53] 0.030863 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:58:07] Energy consumed for RAM : 0.001777 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:58:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524758072718749 W\n",
      "[codecarbon INFO @ 14:58:08] Energy consumed for All CPU : 0.000935 kWh\n",
      "[codecarbon INFO @ 14:58:08] Energy consumed for all GPUs : 0.029678 kWh. Total GPU Power : 337.0943815986712 W\n",
      "[codecarbon INFO @ 14:58:08] 0.032390 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:58:22] Energy consumed for RAM : 0.001857 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:58:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525783391812503 W\n",
      "[codecarbon INFO @ 14:58:23] Energy consumed for All CPU : 0.000978 kWh\n",
      "[codecarbon INFO @ 14:58:23] Energy consumed for all GPUs : 0.031070 kWh. Total GPU Power : 334.2181748880876 W\n",
      "[codecarbon INFO @ 14:58:23] 0.033905 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:58:37] Energy consumed for RAM : 0.001938 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:58:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525318947937501 W\n",
      "[codecarbon INFO @ 14:58:38] Energy consumed for All CPU : 0.001020 kWh\n",
      "[codecarbon INFO @ 14:58:38] Energy consumed for all GPUs : 0.032468 kWh. Total GPU Power : 335.4554887770843 W\n",
      "[codecarbon INFO @ 14:58:38] 0.035426 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:58:38] 0.020714 g.CO2eq/s mean an estimation of 653.2263273441608 kg.CO2eq/year\n",
      "[codecarbon INFO @ 14:58:52] Energy consumed for RAM : 0.002018 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:58:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524250920749997 W\n",
      "[codecarbon INFO @ 14:58:53] Energy consumed for All CPU : 0.001062 kWh\n",
      "[codecarbon INFO @ 14:58:53] Energy consumed for all GPUs : 0.033865 kWh. Total GPU Power : 335.4530092771246 W\n",
      "[codecarbon INFO @ 14:58:53] 0.036945 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:59:07] Energy consumed for RAM : 0.002099 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:59:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52659082934375 W\n",
      "[codecarbon INFO @ 14:59:08] Energy consumed for All CPU : 0.001105 kWh\n",
      "[codecarbon INFO @ 14:59:08] Energy consumed for all GPUs : 0.035264 kWh. Total GPU Power : 335.3986051144568 W\n",
      "[codecarbon INFO @ 14:59:08] 0.038467 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:59:22] Energy consumed for RAM : 0.002179 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:59:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524659196187503 W\n",
      "[codecarbon INFO @ 14:59:23] Energy consumed for All CPU : 0.001147 kWh\n",
      "[codecarbon INFO @ 14:59:23] Energy consumed for all GPUs : 0.036662 kWh. Total GPU Power : 335.88801122309764 W\n",
      "[codecarbon INFO @ 14:59:23] 0.039989 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:59:37] Energy consumed for RAM : 0.002260 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:59:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52445838959375 W\n",
      "[codecarbon INFO @ 14:59:38] Energy consumed for All CPU : 0.001189 kWh\n",
      "[codecarbon INFO @ 14:59:38] Energy consumed for all GPUs : 0.038059 kWh. Total GPU Power : 335.1641616609822 W\n",
      "[codecarbon INFO @ 14:59:38] 0.041508 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:59:52] Energy consumed for RAM : 0.002340 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:59:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525466379750004 W\n",
      "[codecarbon INFO @ 14:59:53] Energy consumed for All CPU : 0.001232 kWh\n",
      "[codecarbon INFO @ 14:59:53] Energy consumed for all GPUs : 0.039448 kWh. Total GPU Power : 333.56098535067014 W\n",
      "[codecarbon INFO @ 14:59:53] 0.043020 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:00:07] Energy consumed for RAM : 0.002421 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:00:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525195973906252 W\n",
      "[codecarbon INFO @ 15:00:08] Energy consumed for All CPU : 0.001274 kWh\n",
      "[codecarbon INFO @ 15:00:08] Energy consumed for all GPUs : 0.040838 kWh. Total GPU Power : 333.8079615978285 W\n",
      "[codecarbon INFO @ 15:00:08] 0.044533 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:00:23] Energy consumed for RAM : 0.002502 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:00:23] Delta energy consumed for CPU with cpu_load : 0.000043 kWh, power : 10.525929158062498 W\n",
      "[codecarbon INFO @ 15:00:23] Energy consumed for All CPU : 0.001317 kWh\n",
      "[codecarbon INFO @ 15:00:23] Energy consumed for all GPUs : 0.042249 kWh. Total GPU Power : 335.4852564997877 W\n",
      "[codecarbon INFO @ 15:00:23] 0.046068 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:00:38] Energy consumed for RAM : 0.002583 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:00:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525675520062501 W\n",
      "[codecarbon INFO @ 15:00:38] Energy consumed for All CPU : 0.001359 kWh\n",
      "[codecarbon INFO @ 15:00:38] Energy consumed for all GPUs : 0.043647 kWh. Total GPU Power : 335.71832186812014 W\n",
      "[codecarbon INFO @ 15:00:38] 0.047589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:00:38] 0.020781 g.CO2eq/s mean an estimation of 655.3387158770382 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:00:53] Energy consumed for RAM : 0.002663 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:00:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5261544815 W\n",
      "[codecarbon INFO @ 15:00:53] Energy consumed for All CPU : 0.001402 kWh\n",
      "[codecarbon INFO @ 15:00:53] Energy consumed for all GPUs : 0.045042 kWh. Total GPU Power : 334.9648884294565 W\n",
      "[codecarbon INFO @ 15:00:53] 0.049107 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:01:08] Energy consumed for RAM : 0.002744 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:01:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5242371355625 W\n",
      "[codecarbon INFO @ 15:01:08] Energy consumed for All CPU : 0.001444 kWh\n",
      "[codecarbon INFO @ 15:01:08] Energy consumed for all GPUs : 0.046436 kWh. Total GPU Power : 334.29177454729415 W\n",
      "[codecarbon INFO @ 15:01:08] 0.050624 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:01:23] Energy consumed for RAM : 0.002824 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:01:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52548413984375 W\n",
      "[codecarbon INFO @ 15:01:23] Energy consumed for All CPU : 0.001486 kWh\n",
      "[codecarbon INFO @ 15:01:23] Energy consumed for all GPUs : 0.047828 kWh. Total GPU Power : 334.2712562696959 W\n",
      "[codecarbon INFO @ 15:01:23] 0.052138 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:01:38] Energy consumed for RAM : 0.002905 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:01:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.522616992125002 W\n",
      "[codecarbon INFO @ 15:01:38] Energy consumed for All CPU : 0.001529 kWh\n",
      "[codecarbon INFO @ 15:01:38] Energy consumed for all GPUs : 0.049200 kWh. Total GPU Power : 329.35199470010036 W\n",
      "[codecarbon INFO @ 15:01:38] 0.053633 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:01:53] Energy consumed for RAM : 0.002985 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:01:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524357493125002 W\n",
      "[codecarbon INFO @ 15:01:53] Energy consumed for All CPU : 0.001571 kWh\n",
      "[codecarbon INFO @ 15:01:53] Energy consumed for all GPUs : 0.050584 kWh. Total GPU Power : 332.16860022035667 W\n",
      "[codecarbon INFO @ 15:01:53] 0.055140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:02:08] Energy consumed for RAM : 0.003066 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:02:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525405710750002 W\n",
      "[codecarbon INFO @ 15:02:08] Energy consumed for All CPU : 0.001613 kWh\n",
      "[codecarbon INFO @ 15:02:08] Energy consumed for all GPUs : 0.051977 kWh. Total GPU Power : 334.3355497298936 W\n",
      "[codecarbon INFO @ 15:02:08] 0.056656 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:02:23] Energy consumed for RAM : 0.003146 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:02:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524094168875003 W\n",
      "[codecarbon INFO @ 15:02:23] Energy consumed for All CPU : 0.001656 kWh\n",
      "[codecarbon INFO @ 15:02:23] Energy consumed for all GPUs : 0.053370 kWh. Total GPU Power : 334.6952205187244 W\n",
      "[codecarbon INFO @ 15:02:23] 0.058172 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:02:38] Energy consumed for RAM : 0.003227 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:02:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52443567415625 W\n",
      "[codecarbon INFO @ 15:02:38] Energy consumed for All CPU : 0.001698 kWh\n",
      "[codecarbon INFO @ 15:02:38] Energy consumed for all GPUs : 0.054781 kWh. Total GPU Power : 338.66756373063475 W\n",
      "[codecarbon INFO @ 15:02:38] 0.059706 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:02:38] 0.020727 g.CO2eq/s mean an estimation of 653.6323481983254 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:02:53] Energy consumed for RAM : 0.003307 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:02:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5250049480625 W\n",
      "[codecarbon INFO @ 15:02:53] Energy consumed for All CPU : 0.001741 kWh\n",
      "[codecarbon INFO @ 15:02:53] Energy consumed for all GPUs : 0.056178 kWh. Total GPU Power : 335.34371292494035 W\n",
      "[codecarbon INFO @ 15:02:53] 0.061226 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:03:08] Energy consumed for RAM : 0.003388 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:03:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52386168115625 W\n",
      "[codecarbon INFO @ 15:03:08] Energy consumed for All CPU : 0.001783 kWh\n",
      "[codecarbon INFO @ 15:03:08] Energy consumed for all GPUs : 0.057583 kWh. Total GPU Power : 337.21940421632655 W\n",
      "[codecarbon INFO @ 15:03:08] 0.062753 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:03:23] Energy consumed for RAM : 0.003468 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:03:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5245261268 W\n",
      "[codecarbon INFO @ 15:03:23] Energy consumed for All CPU : 0.001825 kWh\n",
      "[codecarbon INFO @ 15:03:23] Energy consumed for all GPUs : 0.058981 kWh. Total GPU Power : 335.54078814616037 W\n",
      "[codecarbon INFO @ 15:03:23] 0.064274 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:03:38] Energy consumed for RAM : 0.003549 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:03:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524433441593752 W\n",
      "[codecarbon INFO @ 15:03:38] Energy consumed for All CPU : 0.001868 kWh\n",
      "[codecarbon INFO @ 15:03:38] Energy consumed for all GPUs : 0.060388 kWh. Total GPU Power : 337.8863223596594 W\n",
      "[codecarbon INFO @ 15:03:38] 0.065805 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:03:53] Energy consumed for RAM : 0.003629 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:03:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524956292375 W\n",
      "[codecarbon INFO @ 15:03:53] Energy consumed for All CPU : 0.001910 kWh\n",
      "[codecarbon INFO @ 15:03:53] Energy consumed for all GPUs : 0.061792 kWh. Total GPU Power : 337.09917448418656 W\n",
      "[codecarbon INFO @ 15:03:53] 0.067332 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:04:08] Energy consumed for RAM : 0.003710 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:04:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52429727890625 W\n",
      "[codecarbon INFO @ 15:04:08] Energy consumed for All CPU : 0.001952 kWh\n",
      "[codecarbon INFO @ 15:04:08] Energy consumed for all GPUs : 0.063205 kWh. Total GPU Power : 339.0410282031501 W\n",
      "[codecarbon INFO @ 15:04:08] 0.068867 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:04:23] Energy consumed for RAM : 0.003790 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:04:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.524239509875002 W\n",
      "[codecarbon INFO @ 15:04:23] Energy consumed for All CPU : 0.001995 kWh\n",
      "[codecarbon INFO @ 15:04:23] Energy consumed for all GPUs : 0.064605 kWh. Total GPU Power : 336.0368482384821 W\n",
      "[codecarbon INFO @ 15:04:23] 0.070390 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:04:38] Energy consumed for RAM : 0.003871 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:04:38] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525727542312502 W\n",
      "[codecarbon INFO @ 15:04:38] Energy consumed for All CPU : 0.002037 kWh\n",
      "[codecarbon INFO @ 15:04:38] Energy consumed for all GPUs : 0.066015 kWh. Total GPU Power : 338.48064030881886 W\n",
      "[codecarbon INFO @ 15:04:38] 0.071923 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:04:38] 0.020897 g.CO2eq/s mean an estimation of 658.9998373874776 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:04:53] Energy consumed for RAM : 0.003951 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:04:53] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52491716346875 W\n",
      "[codecarbon INFO @ 15:04:53] Energy consumed for All CPU : 0.002080 kWh\n",
      "[codecarbon INFO @ 15:04:53] Energy consumed for all GPUs : 0.067424 kWh. Total GPU Power : 335.8225357622276 W\n",
      "[codecarbon INFO @ 15:04:53] 0.073455 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:05:08] Energy consumed for RAM : 0.004031 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:05:08] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52430635090625 W\n",
      "[codecarbon INFO @ 15:05:08] Energy consumed for All CPU : 0.002122 kWh\n",
      "[codecarbon INFO @ 15:05:08] Energy consumed for all GPUs : 0.068829 kWh. Total GPU Power : 339.7957490765913 W\n",
      "[codecarbon INFO @ 15:05:08] 0.074982 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:05:23] Energy consumed for RAM : 0.004112 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:05:23] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52502557859375 W\n",
      "[codecarbon INFO @ 15:05:23] Energy consumed for All CPU : 0.002164 kWh\n",
      "[codecarbon INFO @ 15:05:23] Energy consumed for all GPUs : 0.070237 kWh. Total GPU Power : 337.98043660950106 W\n",
      "[codecarbon INFO @ 15:05:23] 0.076513 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:05:30] Energy consumed for RAM : 0.004152 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 15:05:31] Delta energy consumed for CPU with cpu_load : 0.000021 kWh, power : 10.516330188000001 W\n",
      "[codecarbon INFO @ 15:05:31] Energy consumed for All CPU : 0.002185 kWh\n",
      "[codecarbon INFO @ 15:05:31] Energy consumed for all GPUs : 0.070920 kWh. Total GPU Power : 314.99311407125936 W\n",
      "[codecarbon INFO @ 15:05:31] 0.077258 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1428, training_loss=0.7412545624233428, metrics={'train_runtime': 774.1957, 'train_samples_per_second': 29.489, 'train_steps_per_second': 1.844, 'total_flos': 2.513865965174784e+16, 'train_loss': 0.7412545624233428, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d15764a-583b-45c9-9d97-cb8e0594fea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuning complete. LoRA weights saved.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./qwen2-0.5B-lora-finetuned-breastcancer\")\n",
    "print(\"✅ Fine-tuning complete. LoRA weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85a6c182-ae6d-44c6-93e1-9a288d83df41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport evaluate\\n\\n# Load BLEU metric\\nbleu = evaluate.load(\"bleu\")\\n\\npredictions = []\\nreferences = []\\n\\nfor idx in range(len(val_dataset)):\\n    input_ids = val_dataset[idx][\"input_ids\"].unsqueeze(0).to(model.device)\\n    attention_mask = val_dataset[idx][\"attention_mask\"].unsqueeze(0).to(model.device)\\n\\n    with torch.no_grad():\\n        output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512)\\n\\n    pred_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\\n    ref_text = tokenizer.decode(val_dataset[idx][\"labels\"], skip_special_tokens=True)\\n\\n    predictions.append(pred_text)       # <-- full string\\n    references.append([ref_text])       # <-- list of strings\\n\\n# Compute BLEU\\nbleu_score = bleu.compute(predictions=predictions, references=references)\\nprint(\"BLEU score:\", bleu_score[\"bleu\"])\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for idx in range(len(val_dataset)):\n",
    "    input_ids = val_dataset[idx][\"input_ids\"].unsqueeze(0).to(model.device)\n",
    "    attention_mask = val_dataset[idx][\"attention_mask\"].unsqueeze(0).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512)\n",
    "\n",
    "    pred_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    ref_text = tokenizer.decode(val_dataset[idx][\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(pred_text)       # <-- full string\n",
    "    references.append([ref_text])       # <-- list of strings\n",
    "\n",
    "# Compute BLEU\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_score[\"bleu\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9ccb7ba-b7f3-427c-b78a-bb747b80c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.9771004697738882\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "\n",
    "# DataLoader to batch the validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "predictions, references = [], []\n",
    "\n",
    "for batch in val_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    # Decode predictions\n",
    "    batch_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Decode references (fix -100s first)\n",
    "    batch_refs = []\n",
    "    for labels in batch[\"labels\"]:\n",
    "        labels = [tok if tok != -100 else tokenizer.pad_token_id for tok in labels]\n",
    "        batch_refs.append([tokenizer.decode(labels, skip_special_tokens=True)])\n",
    "\n",
    "    predictions.extend(batch_preds)\n",
    "    references.extend(batch_refs)\n",
    "\n",
    "# Compute BLEU\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_score[\"bleu\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94125b-1705-498d-af17-ef2565268323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen32b)",
   "language": "python",
   "name": "qwen32b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
