{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69221af6-4c46-4371-a209-11e5f428e6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 14:02:17.684395: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-04 14:02:17.684418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-04 14:02:17.685200: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-04 14:02:17.689284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-04 14:02:18.205349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafe36570b99435eaceb5f4e3e95b1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_353319/1006941263.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[codecarbon WARNING @ 14:02:22] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 14:02:22] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 14:02:22] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 14:02:23] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 14:02:23] CPU Model on constant consumption mode: AMD Ryzen 7 5800X 8-Core Processor\n",
      "[codecarbon WARNING @ 14:02:23] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 14:02:23] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 14:02:23] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 14:02:23] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 14:02:23] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 14:02:23]   Platform system: Linux-5.15.0-153-generic-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 14:02:23]   Python version: 3.10.18\n",
      "[codecarbon INFO @ 14:02:23]   CodeCarbon version: 3.0.5\n",
      "[codecarbon INFO @ 14:02:23]   Available RAM : 62.701 GB\n",
      "[codecarbon INFO @ 14:02:23]   CPU count: 16 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 14:02:23]   CPU model: AMD Ryzen 7 5800X 8-Core Processor\n",
      "[codecarbon INFO @ 14:02:23]   GPU count: 2\n",
      "[codecarbon INFO @ 14:02:23]   GPU model: 2 x NVIDIA GeForce RTX 3090\n",
      "[codecarbon INFO @ 14:02:27] Emissions data (if any) will be saved to file /home/bobby/Cancer4VN/qwen-sft-local-test/emissions.csv\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobbyhieubui\u001b[0m (\u001b[33mbobbyhieubui-university-of-wisconsin-madison\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bobby/Cancer4VN/wandb/run-20251004_140228-vpmmaeao</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/vpmmaeao' target=\"_blank\">light-capybara-8</a></strong> to <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface' target=\"_blank\">https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/vpmmaeao' target=\"_blank\">https://wandb.ai/bobbyhieubui-university-of-wisconsin-madison/huggingface/runs/vpmmaeao</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 02:27, Epoch 500/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 14:02:44] Energy consumed for RAM : 0.000086 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:02:44] Delta energy consumed for CPU with cpu_load : 0.000045 kWh, power : 10.525426069200002 W\n",
      "[codecarbon INFO @ 14:02:44] Energy consumed for All CPU : 0.000045 kWh\n",
      "[codecarbon INFO @ 14:02:44] Energy consumed for all GPUs : 0.001335 kWh. Total GPU Power : 300.2258932925965 W\n",
      "[codecarbon INFO @ 14:02:44] 0.001466 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:02:59] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:02:59] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.529583154125001 W\n",
      "[codecarbon INFO @ 14:02:59] Energy consumed for All CPU : 0.000088 kWh\n",
      "[codecarbon INFO @ 14:02:59] Energy consumed for all GPUs : 0.002556 kWh. Total GPU Power : 293.14588434984665 W\n",
      "[codecarbon INFO @ 14:02:59] 0.002810 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:03:14] Energy consumed for RAM : 0.000247 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:03:14] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.540539690843753 W\n",
      "[codecarbon INFO @ 14:03:14] Energy consumed for All CPU : 0.000130 kWh\n",
      "[codecarbon INFO @ 14:03:14] Energy consumed for all GPUs : 0.003876 kWh. Total GPU Power : 316.89033515990957 W\n",
      "[codecarbon INFO @ 14:03:14] 0.004253 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:03:29] Energy consumed for RAM : 0.000328 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:03:29] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.528556955 W\n",
      "[codecarbon INFO @ 14:03:29] Energy consumed for All CPU : 0.000173 kWh\n",
      "[codecarbon INFO @ 14:03:29] Energy consumed for all GPUs : 0.005120 kWh. Total GPU Power : 298.73395975047015 W\n",
      "[codecarbon INFO @ 14:03:29] 0.005621 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:03:44] Energy consumed for RAM : 0.000408 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:03:44] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.52526784115625 W\n",
      "[codecarbon INFO @ 14:03:44] Energy consumed for All CPU : 0.000215 kWh\n",
      "[codecarbon INFO @ 14:03:44] Energy consumed for all GPUs : 0.006454 kWh. Total GPU Power : 320.1036379819908 W\n",
      "[codecarbon INFO @ 14:03:44] 0.007077 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:03:59] Energy consumed for RAM : 0.000489 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:03:59] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.531889509312501 W\n",
      "[codecarbon INFO @ 14:03:59] Energy consumed for All CPU : 0.000257 kWh\n",
      "[codecarbon INFO @ 14:03:59] Energy consumed for all GPUs : 0.007715 kWh. Total GPU Power : 302.83134892798523 W\n",
      "[codecarbon INFO @ 14:03:59] 0.008461 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:04:14] Energy consumed for RAM : 0.000569 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:04:14] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.525005125249999 W\n",
      "[codecarbon INFO @ 14:04:14] Energy consumed for All CPU : 0.000300 kWh\n",
      "[codecarbon INFO @ 14:04:14] Energy consumed for all GPUs : 0.009052 kWh. Total GPU Power : 320.8841111715956 W\n",
      "[codecarbon INFO @ 14:04:14] 0.009921 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:04:29] Energy consumed for RAM : 0.000650 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:04:29] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.5310314020625 W\n",
      "[codecarbon INFO @ 14:04:29] Energy consumed for All CPU : 0.000342 kWh\n",
      "[codecarbon INFO @ 14:04:29] Energy consumed for all GPUs : 0.010297 kWh. Total GPU Power : 298.76377559567817 W\n",
      "[codecarbon INFO @ 14:04:29] 0.011288 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:04:29] 0.019148 g.CO2eq/s mean an estimation of 603.862783717655 kg.CO2eq/year\n",
      "[codecarbon INFO @ 14:04:44] Energy consumed for RAM : 0.000730 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:04:44] Delta energy consumed for CPU with cpu_load : 0.000042 kWh, power : 10.534170798375003 W\n",
      "[codecarbon INFO @ 14:04:44] Energy consumed for All CPU : 0.000385 kWh\n",
      "[codecarbon INFO @ 14:04:44] Energy consumed for all GPUs : 0.011641 kWh. Total GPU Power : 322.681126940806 W\n",
      "[codecarbon INFO @ 14:04:44] 0.012756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 14:04:56] Energy consumed for RAM : 0.000797 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 14:04:57] Delta energy consumed for CPU with cpu_load : 0.000035 kWh, power : 10.521170907692307 W\n",
      "[codecarbon INFO @ 14:04:57] Energy consumed for All CPU : 0.000420 kWh\n",
      "[codecarbon INFO @ 14:04:57] Energy consumed for all GPUs : 0.012643 kWh. Total GPU Power : 286.51721776340435 W\n",
      "[codecarbon INFO @ 14:04:57] 0.013860 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.019785152435302735, metrics={'train_runtime': 148.3454, 'train_samples_per_second': 26.964, 'train_steps_per_second': 3.371, 'total_flos': 23621275392000.0, 'train_loss': 0.019785152435302735, 'epoch': 500.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ========================\n",
    "# 1️⃣ Paths and model\n",
    "# ========================\n",
    "BASE_MODEL = \"Qwen/Qwen2-0.5B\"  # path to original model\n",
    "OUTPUT_DIR = \"./qwen-sft-local-test\"  # local folder for checkpoints\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ========================\n",
    "# 2️⃣ Load tokenizer & model\n",
    "# ========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,  # mixed precision\n",
    ").cuda()\n",
    "\n",
    "# ========================\n",
    "# 3️⃣ Prepare your dataset\n",
    "# ========================\n",
    "# Example format: [{\"input\": \"Question: ...\\nAnswer:\", \"output\": \" your answer\"}]\n",
    "data = [\n",
    "    {\"input\": \"What is lung cancer?\\nAnswer:\", \"output\": \" Lung cancer is ...\"},\n",
    "    {\"input\": \"Is smoking harmful?\\nAnswer:\", \"output\": \" Yes, smoking is ...\"}\n",
    "]\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    # Concatenate input and output as one sequence\n",
    "    full_text = example[\"input\"] + example[\"output\"]\n",
    "    tokens = tokenizer(full_text, truncation=True, max_length=1024)\n",
    "    return tokens\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset = dataset.map(tokenize_fn, batched=False)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# ========================\n",
    "# 4️⃣ Data collator\n",
    "# ========================\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # causal LM\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# 5️⃣ Training arguments\n",
    "# ========================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,  # small batch for stability\n",
    "    gradient_accumulation_steps=4,  # effective batch size = 4\n",
    "    learning_rate=1e-5,             # start small\n",
    "    max_steps=500,                   # short debug run\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=10,\n",
    "    report_to=None,                  # no wandb/other reporting\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,     # save memory\n",
    "    max_grad_norm=1.0,               # gradient clipping\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# 6️⃣ Trainer\n",
    "# ========================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# 7️⃣ Start training\n",
    "# ========================\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121fb04-b11a-4fb3-bb61-96d58842cd36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen32b)",
   "language": "python",
   "name": "qwen32b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
