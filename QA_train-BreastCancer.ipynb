{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb134fd0-d917-480b-9080-5d3c5b51a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.cuda.get_device_name(0))  # GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c093eb4-bfdc-4def-b9da-a6750c300e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0deb7180-9d17-44c7-978a-013869b3b3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36800818</td>\n",
       "      <td>Evaluating the effect of nurses supportive and...</td>\n",
       "      <td>Endometriosis is one of the womens aggressive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36800748</td>\n",
       "      <td>18F-FDG PET-CT in Cutaneous Metastases in Brea...</td>\n",
       "      <td>Cutaneous and subcutaneous metastases are an e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36800716</td>\n",
       "      <td>Efficacy of pre - operative axillary ultrasono...</td>\n",
       "      <td>To determine false negative rate, negative pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36800710</td>\n",
       "      <td>Is complete pathological response truly a comp...</td>\n",
       "      <td>To check if complete pathological response in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36800640</td>\n",
       "      <td>Experiences of a Multiethnic Cohort of Patient...</td>\n",
       "      <td>Financial reimbursement programs (FRPs) offset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9508</th>\n",
       "      <td>36194067</td>\n",
       "      <td>The Lumbar Artery Perforator Flap in Breast Re...</td>\n",
       "      <td>The lumbar artery perforator flap is a valuabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9509</th>\n",
       "      <td>36194066</td>\n",
       "      <td>Sensory Restoration in Abdominally Based Free ...</td>\n",
       "      <td>Neurotization in breast reconstruction can be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9510</th>\n",
       "      <td>31915335</td>\n",
       "      <td>StatPearls</td>\n",
       "      <td>Breast lumps or masses are very common, partic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9511</th>\n",
       "      <td>26389406</td>\n",
       "      <td>PDQ Cancer Information Summaries</td>\n",
       "      <td>This PDQ cancer information summary has curren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9512</th>\n",
       "      <td>26389160</td>\n",
       "      <td>PDQ Cancer Information Summaries</td>\n",
       "      <td>This PDQ cancer information summary has curren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9513 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pubmed_id                                              title  \\\n",
       "0      36800818  Evaluating the effect of nurses supportive and...   \n",
       "1      36800748  18F-FDG PET-CT in Cutaneous Metastases in Brea...   \n",
       "2      36800716  Efficacy of pre - operative axillary ultrasono...   \n",
       "3      36800710  Is complete pathological response truly a comp...   \n",
       "4      36800640  Experiences of a Multiethnic Cohort of Patient...   \n",
       "...         ...                                                ...   \n",
       "9508   36194067  The Lumbar Artery Perforator Flap in Breast Re...   \n",
       "9509   36194066  Sensory Restoration in Abdominally Based Free ...   \n",
       "9510   31915335                                         StatPearls   \n",
       "9511   26389406                   PDQ Cancer Information Summaries   \n",
       "9512   26389160                   PDQ Cancer Information Summaries   \n",
       "\n",
       "                                               abstract  \n",
       "0     Endometriosis is one of the womens aggressive ...  \n",
       "1     Cutaneous and subcutaneous metastases are an e...  \n",
       "2     To determine false negative rate, negative pre...  \n",
       "3     To check if complete pathological response in ...  \n",
       "4     Financial reimbursement programs (FRPs) offset...  \n",
       "...                                                 ...  \n",
       "9508  The lumbar artery perforator flap is a valuabl...  \n",
       "9509  Neurotization in breast reconstruction can be ...  \n",
       "9510  Breast lumps or masses are very common, partic...  \n",
       "9511  This PDQ cancer information summary has curren...  \n",
       "9512  This PDQ cancer information summary has curren...  \n",
       "\n",
       "[9513 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hf://datasets/Gaborandi/breast_cancer_pubmed_abstracts/breast_cancer.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0c3541-85a3-4d9e-a541-6256fbd78dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (7610, 3)\n",
      "Shape of validation set: (951, 3)\n",
      "Shape of test set: (952, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Shape of training set: {train_df.shape}\")\n",
    "print(f\"Shape of validation set: {validation_df.shape}\")\n",
    "print(f\"Shape of test set: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b853c59a-27bf-4bbc-a8cd-f96e80c2bd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nimport pandas as pd\\nfrom tqdm import tqdm\\n\\n# Load model + tokenizer\\nmodel_id = \"openai/gpt-oss-20b\"\\n#model_id = \"openai/gpt-oss-6.9b\"\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    torch_dtype=torch.float32  \\n)\\n\\n# Assume test_df has a column \"prompt\"\\nprompts = test_df[\"prompt\"].tolist()\\n\\noutputs = []\\nbatch_size = 2   # adjust depending on your GPU memory\\n\\nfor i in tqdm(range(0, len(prompts), batch_size)):\\n    batch_prompts = prompts[i:i+batch_size]\\n    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\\n    \\n    out_tokens = model.generate(\\n        **inputs,\\n        max_new_tokens=256,\\n        temperature=0.7,\\n        do_sample=True,\\n        pad_token_id=tokenizer.eos_token_id,\\n    )\\n    \\n    decoded_batch = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\\n    outputs.extend(decoded_batch)\\n\\n# Add results to DataFrame\\ntest_df[\"model_output\"] = outputs\\ntest_df.to_csv(\"test_with_outputs.csv\", index=False)\\n\\nprint(\"✅ Finished inference. Results saved to test_with_outputs.csv\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model + tokenizer\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "#model_id = \"openai/gpt-oss-6.9b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32  \n",
    ")\n",
    "\n",
    "# Assume test_df has a column \"prompt\"\n",
    "prompts = test_df[\"prompt\"].tolist()\n",
    "\n",
    "outputs = []\n",
    "batch_size = 2   # adjust depending on your GPU memory\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    out_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    decoded_batch = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\n",
    "    outputs.extend(decoded_batch)\n",
    "\n",
    "# Add results to DataFrame\n",
    "test_df[\"model_output\"] = outputs\n",
    "test_df.to_csv(\"test_with_outputs.csv\", index=False)\n",
    "\n",
    "print(\"✅ Finished inference. Results saved to test_with_outputs.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deaa7a6b-dfa7-4d15-a1f9-77cc8c52d9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nimport pandas as pd\\nfrom tqdm import tqdm\\n\\n# Load model + tokenizer\\nmodel_id = \"openai/gpt-oss-20b\"\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    torch_dtype=torch.float32,   # use float32 if CPU only\\n)\\n\\n# Assume test_df has a column \"prompt\"\\nprompts = test_df[\"prompt\"].tolist()\\n\\noutputs = []\\nbatch_size = 2   # adjust depending on your GPU memory\\n\\nfor i in tqdm(range(0, len(prompts), batch_size)):\\n    batch_prompts = prompts[i:i+batch_size]\\n    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\\n    \\n    out_tokens = model.generate(\\n        **inputs,\\n        max_new_tokens=256,\\n        temperature=0.7,\\n        do_sample=True,\\n        pad_token_id=tokenizer.eos_token_id,\\n    )\\n    \\n    decoded_batch = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\\n    outputs.extend(decoded_batch)\\n\\n# Add results to DataFrame\\ntest_df[\"model_output\"] = outputs\\ntest_df.to_csv(\"test_with_outputs.csv\", index=False)\\n\\nprint(\"✅ Finished inference. Results saved to test_with_outputs.csv\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model + tokenizer\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,   # use float32 if CPU only\n",
    ")\n",
    "\n",
    "# Assume test_df has a column \"prompt\"\n",
    "prompts = test_df[\"prompt\"].tolist()\n",
    "\n",
    "outputs = []\n",
    "batch_size = 2   # adjust depending on your GPU memory\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    out_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    decoded_batch = tokenizer.batch_decode(out_tokens, skip_special_tokens=True)\n",
    "    outputs.extend(decoded_batch)\n",
    "\n",
    "# Add results to DataFrame\n",
    "test_df[\"model_output\"] = outputs\n",
    "test_df.to_csv(\"test_with_outputs.csv\", index=False)\n",
    "\n",
    "print(\"✅ Finished inference. Results saved to test_with_outputs.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99296b6f-a0f9-4c51-bf42-194676b81aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\\nimport pandas as pd\\nfrom tqdm import tqdm\\n\\n# Load your test dataset\\n# test_df = pd.read_csv(\"your_test_file.csv\")  # make sure test_df has a column \"prompt\"\\n\\n# Model ID\\nmodel_id = \"EleutherAI/gpt-j-6B\"\\n\\n# Initialize tokenizer and model pipeline\\ngenerator = pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    device=-1  # -1 means CPU\\n)\\n\\nprompts = test_df[\"input\"].tolist()\\noutputs = []\\n\\n# Generate outputs for each prompt\\nfor prompt in tqdm(prompts):\\n    result = generator(\\n        prompt,\\n        max_length=256,   # adjust as needed\\n        do_sample=True,\\n        temperature=0.7\\n    )\\n    outputs.append(result[0][\\'generated_text\\'])\\n\\n# Save results to DataFrame\\ntest_df[\"model_output\"] = outputs\\ntest_df.to_csv(\"test_with_outputs.csv\", index=False)\\nprint(\"✅ Finished inference, results saved to test_with_outputs.csv\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your test dataset\n",
    "# test_df = pd.read_csv(\"your_test_file.csv\")  # make sure test_df has a column \"prompt\"\n",
    "\n",
    "# Model ID\n",
    "model_id = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "# Initialize tokenizer and model pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device=-1  # -1 means CPU\n",
    ")\n",
    "\n",
    "prompts = test_df[\"input\"].tolist()\n",
    "outputs = []\n",
    "\n",
    "# Generate outputs for each prompt\n",
    "for prompt in tqdm(prompts):\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=256,   # adjust as needed\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    outputs.append(result[0]['generated_text'])\n",
    "\n",
    "# Save results to DataFrame\n",
    "test_df[\"model_output\"] = outputs\n",
    "test_df.to_csv(\"test_with_outputs.csv\", index=False)\n",
    "print(\"✅ Finished inference, results saved to test_with_outputs.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d72a2b2-c132-4bc5-aa10-ea65a3b4a359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "  8%|███▌                                      | 10/119 [03:07<33:31, 18.45s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|█████████████████████████████████████████| 119/119 [36:39<00:00, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished inference on GPU using accelerate. Results saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "model_id = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "# Load tokenizer + model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # FP16 for GPU\n",
    "    device_map=\"auto\"           # let accelerate handle GPU placement\n",
    ")\n",
    "\n",
    "# Do NOT set device here — the model already knows where to go\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Batched inference\n",
    "prompts = test_df[\"title\"].tolist()\n",
    "prompts = [str(p) for p in prompts]  # convert everything to string\n",
    "\n",
    "outputs = []\n",
    "batch_size = 8\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    batch_prompts = prompts[i:i+batch_size]\n",
    "    results = generator(\n",
    "        batch_prompts,\n",
    "        max_new_tokens=128,\n",
    "        truncation=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # results is a list of lists, one list per prompt\n",
    "    for r in results:\n",
    "        outputs.append(r[0][\"generated_text\"])\n",
    "\n",
    "# Save outputs\n",
    "test_df[\"model_output\"] = outputs\n",
    "test_df.to_csv(\"test_with_qwen_outputs_breast_cancer.csv\", index=False)\n",
    "print(\"✅ Finished inference on GPU using accelerate. Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9e66ee2-41f6-43cb-98fc-f98273e7183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 2.15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "test_df = pd.read_csv(\"test_with_qwen_outputs_breast_cancer.csv\")\n",
    "\n",
    "references = [[str(ref).split()] for ref in test_df[\"abstract\"].tolist()]\n",
    "hypotheses = [str(hyp).split() for hyp in test_df[\"model_output\"].tolist()]\n",
    "\n",
    "#references = [[ref.split()] for ref in test_df[\"abstract\"].tolist()]  # each ref must be a list of tokens\n",
    "#hypotheses = [hyp.split() for hyp in test_df[\"model_output\"].tolist()\n",
    "\n",
    "# Smoothing function helps with short sentences\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "# Compute corpus BLEU\n",
    "bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth)\n",
    "print(f\"BLEU score: {bleu_score*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94dfd6-38fd-472f-a901-d43b56057c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39130786-98df-4b87-b7f3-6c9afb2c9b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb7b04-2d1e-444a-95be-1bd570013e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57d2d1-56f1-4a10-9ac4-c0c1069d3b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9302758-984a-4daa-bec7-53879e9f49ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13ec2e-0126-43f5-b002-fee8323a6e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6995adc6-de9b-4f3a-950e-4aa4a9664668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qwen32b)",
   "language": "python",
   "name": "qwen32b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
